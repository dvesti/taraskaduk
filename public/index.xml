<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Taras Kaduk on Taras Kaduk</title>
    <link>/</link>
    <description>Recent content in Taras Kaduk on Taras Kaduk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Imposterism in Data Science: Addressing the credentials problem</title>
      <link>/2018/01/22/impostor/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/22/impostor/</guid>
      <description>&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/impostor/fraud.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/imposterism-in-data-science-f96c29ae96ce&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt; &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We happen to talk a lot about the impostor syndrome these days. No wonder — it seems to be an important subject. But what is it? That feeling of faking it while others &lt;em&gt;clearly&lt;/em&gt; know what’s they’re doing.&lt;/p&gt;
&lt;p&gt;Many attempts have been made to clarify the issue. Explaining that it is OK, that we all feel that way going through life. Et cetera et cetera. Some advice has gone as far as making the impostor syndrome a badge of honor. But that’s on the extreme side, and I’ll save this topic for another time…&lt;/p&gt;
&lt;p&gt;Today, I want to pick apart one particular advice on how to combat the feeling of being a fraud. That is, to roll up your sleeves and start learning. I’ve heard it many times before, and always wanted to respond, but never had the time. Because I was, you know, learning.&lt;/p&gt;
&lt;div id=&#34;curb-your-imposterism-start-meta-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Curb your imposterism, start meta-learning?&lt;/h2&gt;
&lt;p&gt;The blog post that triggered this post was written a few days ago by a fellow data scientist &lt;a href=&#34;https://twitter.com/edwin_thoen&#34;&gt;Edwin Thoen&lt;/a&gt; and is titled &lt;a href=&#34;https://edwinth.github.io/meta-learning/&#34;&gt;&lt;em&gt;“Curb your imposterism, start meta-learning”&lt;/em&gt;&lt;/a&gt;. In it, Edwin highlights the importance of choosing what to focus on and what to learn, and then learning it.&lt;/p&gt;
&lt;p&gt;First off, I think this is a great advice in general. In fact, I typically follow somewhat similar guidelines in learning new things. But this in no way helps me combat my impostor syndrome.&lt;/p&gt;
&lt;p&gt;Secondly, I must point out that the author does acknowledge his advice is not universal. Edwin says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Now, what works for me might not work for you. Maybe a different system fits you better. However, I think everybody benefits from defining the data scientist he/she is and actively choose what not to learn. So, what I’m going to say next is not a disagreement with this approach, but rather an important addition that I feel is constantly overlooked.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;do-you-even-phd-bro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Do you even PhD, bro?&lt;/h2&gt;
&lt;p&gt;There are jobs out there where your degree means everything. Lawyers and doctors are a good example of this category. And there are occupations where what matters the most is what you &lt;strong&gt;can&lt;/strong&gt; do, while your degree (or lack of thereof) is a secondary issue at most.&lt;/p&gt;
&lt;p&gt;I find Data Science increasingly leaning towards the former category. Scan the posted jobs for data science related positions: you’ll see a Master’s degree desired and a Bachelor’s degree being a mandatory requirement. Higher position? PhDs please.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;faking-it-in-the-world-of-educated-people&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Faking it in the world of educated people&lt;/h2&gt;
&lt;p&gt;If you think that this is a sound request, you won’t be wrong. But let me speak about it from my own experience. I’ve been in the workforce for 10 years, have been doing analytics for over 5 years. And I don’t have a degree. No, I don’t mean MS or PhD. I don’t have a Bachelor’s degree.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/post/impostor/yachty.gif&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;Yep. And it’s hard to admit. Ever since I had to leave one of the best Economics universities of my country for a full-time job to make ends meet, I felt ashamed of the fact that I never graduated. And the feeling became even stronger as I broke into the analytics / data science field. Whereas overall a Bachelor’s degree in combination with hard work and continuous learning is fairly enough to hold a well-paid job, in our field it is an absolute bare minimum, but you need much more.&lt;/p&gt;
&lt;p&gt;So how do you think it feels when you come across a position you have all the skills for, but which requires an MS in Math, Computer Science or Statistics? You’re like &lt;em&gt;“I’m 2 steps below that!”&lt;/em&gt; Or how does it feel when your direct report whom you teach, mentor and guide has a Master’s degree?&lt;/p&gt;
&lt;p&gt;It doesn’t feel terrible. In fact, it feels just fine. But your impostor syndrome feeds off that.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;no-silver-bullet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;No silver bullet&lt;/h2&gt;
&lt;p&gt;There aren’t many of us. Somehow, data science is different from even computer science. Credentials matter here. And most of the people do have at least a bare minimum of a Bachelor’s degree.&lt;/p&gt;
&lt;div id=&#34;but-for-the-small-amount-of-folks-like-me-who-work-or-want-to-work-in-analytics-data-science-yet-dont-have-any-degree-beyond-high-school-here-is-some-bitter-truth-there-is-no-silver-bullet.-go-and-study-formally.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span style=&#34;background-color: #DDDDDD&#34;&gt; But for the small amount of folks like me, who work (or want to work) in analytics / data science, yet don’t have any degree beyond high school, here is some bitter truth: there is no silver bullet. Go and study formally.&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;All these ten years, I though I can prove the world wrong, thought I can show that one can be smart without a diploma from a 4-year university. MOOCs, online classes, free and paid resources, books, guides… It all takes you only so far.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/post/impostor/homer.gif&#34; style=&#34;width:75.0%&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;And I’m tired of fighting this uphill battle. That’s why I’m starting again from ground zero. Getting an AA at a state college, followed by a BS and hopefully an MS at a state university. And we’ll see what’s next. Classes start next week. It’s going to be long and costly, but it’s going to be worth it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The 4 Stages of Data Analytics Maturity: Challenging the Gartner’s Model</title>
      <link>/2018/01/22/4-stages/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/22/4-stages/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://www.linkedin.com/pulse/4-stages-data-analytics-maturity-challenging-gartners-taras-kaduk/&#34;&gt;&lt;strong&gt;LinkedIn&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://medium.com/taras-kaduk/4-stages-of-data-analytics-maturity-challenging-gartners-model-590eb5ebe6d1&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt; &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you happen to work in analytics, data science or business intelligence, you’ve probably seen one of the iterations of this Gartner’s graph on stages of data analysis in a company:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/4-stages/gartner.jpg&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The figure above shows various stages of analytics maturity, from “descriptive” to “prescriptive”. I’ve seen it so many times, it became an eyesore to me.&lt;/p&gt;
&lt;p&gt;There is nothing wrong with it. This look nicely breaks down the evolution of analytics into understandable parts and pairs each stage with a question to be answered: &lt;em&gt;what happened, why did it happen, what will happen, how can we make it happen&lt;/em&gt;. And exactly this cadence of words &lt;em&gt;what, why, what, how&lt;/em&gt; is what made me think that the relation between the 4 stages is not exactly linear.&lt;/p&gt;
&lt;p&gt;In my mind, the &lt;em&gt;what&lt;/em&gt; questions (descriptive and predictive analytics) can simply be answered by what’s in the data: either existing historical data (descriptive analytics) or historical data, extrapolated into the future using machine learning techniques and forecasting (predictive analytics). You can easily move from one stage to another. There is no “diagnostic analytics” step in between.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Why&lt;/em&gt; and &lt;em&gt;how&lt;/em&gt; (diagnostic and prescriptive analytics), on the other hand, are the questions that can be answered with existing data and a dash of business intelligence, either manual (a person going over the numbers and figuring things out), or baked in (an algorithm analyzing the numbers and producing verdicts based on models ran). In other words, both &lt;strong&gt;diagnostic&lt;/strong&gt; and &lt;strong&gt;prescriptive&lt;/strong&gt; analytics build on top of &lt;strong&gt;descriptive&lt;/strong&gt; and &lt;strong&gt;predictive&lt;/strong&gt; analytics respectively.&lt;/p&gt;
&lt;p&gt;So, another way to visualize the connection between the four times would look something like this:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/4-stages/2by2.png&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;One issue with the following graph is that it doesn’t fully show all the ways that &lt;em&gt;data + insight + machine learning&lt;/em&gt; produce 4 flavors of analytics. And hence the good ol’ venn diagram:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/4-stages/venn.png&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Every company’s approach to analytics and data science is still unique: there are very few best practices known in the industry, and we all are still figuring it out. Similarly, every analyst’s view on data analytics evolution and maturity will be different, and many of my colleagues will disagree with this view. And that is fine. We are still in the early stages of learning how to cook the proverbial spaghetti, and therefore let’s not rob ourselves of the joy of throwing stuff from the pot onto the wall and seeing what sticks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My first shiny app: calculating your hourly rate as a consultant</title>
      <link>/2018/01/13/shiny_rate/</link>
      <pubDate>Sat, 13 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/13/shiny_rate/</guid>
      <description>&lt;div id=&#34;one-year-after-building-it-and-forgetting-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;em&gt;(One year after building it and forgetting it)&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/my-first-shiny-app-calculating-your-hourly-rate-as-a-consultant-97abe5296a37&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Is it just me, or is your hard drive also full of abandoned projects, ideas et cetera? I know I’m not alone.&lt;/p&gt;
&lt;p&gt;Cleaning up my R folder the other day, I stumbled upon a file I hardly remember creating. It is was a Shiny app built to help one calculate the hourly rate of services, given a desired income and an amount of time working (billable hours in a day, days in a week, weeks in a year). I think I built it watching the RStudio intro webinars into Shiny: part 1, part 2 and part 3.&lt;/p&gt;
&lt;p&gt;The idea is that the user inputs all the values, and then the app suggests how to change any of the values to bring the system to the equilibrium.&lt;/p&gt;
&lt;p&gt;Now that I’ve found the file, I’m publishing it as is at &lt;a href=&#34;https://taraskaduk.shinyapps.io/rate/&#34; class=&#34;uri&#34;&gt;https://taraskaduk.shinyapps.io/rate/&lt;/a&gt;. Below is the app, the explanation and the code-through.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The app&lt;/h2&gt;
&lt;p&gt;Here is the app itself, hosted on &lt;a href=&#34;https://www.shinyapps.io/&#34; class=&#34;uri&#34;&gt;https://www.shinyapps.io/&lt;/a&gt;. (LMK if you don’t see the embeded app here: something must have gone wrong) &lt;iframe width=&#34;1000&#34; height=&#34;650&#34; src=&#34;https://taraskaduk.shinyapps.io/rate/&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-it-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does it work&lt;/h2&gt;
&lt;p&gt;I know, I know, good UI needs no explanation. Please pardon my poor design. Finessing the details is something I may work on later. Today, I’m living by “ship daily”, and so I ship my minimum viable product&lt;/p&gt;
&lt;p&gt;So, it works like this. First, you enter your desired figures into the input box. Let’s say, you are guessing a $75/hr rate, and you’re hoping to have 6 billable hours a day, working 5 days a week, 48 weeks a year. You’re hoping to make $100,000 a year in gross income&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/shiny-rate/figures/1.png&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Now looking at the output pane, the result is not too far from our initial estimate, but we’re slightly off, making only $108,000 instead of $120,000: &lt;img src=&#34;/post/shiny-rate/figures/2.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How can we get to $120K? Well, the app suggests a few options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we can bump up our rate to $83/hr&lt;/li&gt;
&lt;li&gt;we can shoot for more than 6 hours billable hours a day (it still says 6, but that’s obviously rounding)&lt;/li&gt;
&lt;li&gt;we could keep the billable hours and the rate the same, but work more days a week&lt;/li&gt;
&lt;li&gt;we can leave all the parameters as they are, but increase the amount of weeks working from 48 to… 53. Bummer, a year only has 52 weeks, therefore this won’t work.&lt;/li&gt;
&lt;li&gt;we could settle for less and keep the $108K&lt;/li&gt;
&lt;li&gt;finally, we could find a compromise by changing every parameter slightly.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moving the needle on any of these parameters will “stabilize” the system. Watch:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/shiny-rate/figures/3.png&#34; alt=&#34;Changing the rate from $75 to $84/hour&#34; style=&#34;width:100.0%&#34; /&gt; &lt;img src=&#34;/post/shiny-rate/figures/3.png&#34; alt=&#34;Changing the rate to $80/hour and working 50 weeks a year&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is pretty much it, in a nutshell. I was interested in making this kind dynamic calculation possible. It took me a while, but it works as expected.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The code&lt;/h2&gt;
&lt;p&gt;Here is a reminder for me: always annotate your code! When I dug it out last week, I could barely understand anything. I still don’t get a solid chunk of it.&lt;/p&gt;
&lt;p&gt;I’m going to annotate it as much as I can, with the time I have. Hopefully you can comb through it.&lt;/p&gt;
&lt;p&gt;Any improvements? Suggestions? It’s all on Github — you know what to do!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/taraskaduk/shiny_app_rate&#34; class=&#34;uri&#34;&gt;https://github.com/taraskaduk/shiny_app_rate&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)
library(tibble)
library(dplyr)
library(shiny)
library(rsconnect)
library(scales)


# UI ----------------------------------------------------------------------

ui &amp;lt;- fluidPage(
  headerPanel(&amp;#39;Hourly rate calculator by Taras Kaduk&amp;#39;),
  sidebarPanel(

    # I should have probably parametirized the mins and maxs.
    sliderInput(&amp;#39;iRate&amp;#39;, &amp;#39;Rate&amp;#39;, 70,
                min = 50, max = 200),
    sliderInput(&amp;#39;iHours&amp;#39;, &amp;#39;Hours&amp;#39;, 8,
                min = 5, max = 12),
    sliderInput(&amp;#39;iDays&amp;#39;, &amp;#39;Days&amp;#39;, 5,
                min = 4 , max = 7),
    sliderInput(&amp;#39;iWeeks&amp;#39;, &amp;#39;Weeks&amp;#39;, 48,
                min = 42, max = 52),
    sliderInput(&amp;#39;iIncome&amp;#39;, &amp;#39;Income&amp;#39;, 145000,
                min = 55000, max = 200000, step = 1000)
  ),
  mainPanel(
    plotOutput(&amp;#39;plot1&amp;#39;, width = 600, height = 450),
    plotOutput(&amp;#39;plot2&amp;#39;, width = 600, height = 100)
  )
)


# Server ------------------------------------------------------------------

server &amp;lt;- function(input, output) {
  
  #These are initial inputs
  iRate &amp;lt;- reactive({input$iRate})
  iHours &amp;lt;- 10
  iDays &amp;lt;- 7
  iWeeks &amp;lt;- 48
  iIncome &amp;lt;- 145000
  
  # Colors
  col_need &amp;lt;- &amp;#39;red4&amp;#39;
  col_value &amp;lt;- &amp;#39;grey30&amp;#39;
  
  # Calculating annual income based on inputs.
  # BTW, what is all this reactive({ }) stuff? Shiny syntax baby!
  calcIncome &amp;lt;- reactive({input$iRate * input$iHours  * input$iDays * input$iWeeks})
  
  # Creating a vector with input variables.
  # Why vector? I don&amp;#39;t remember.
  # Tried switching to a data frame and failed.
  vInput &amp;lt;- reactive({
    c(&amp;quot;Rate&amp;quot; = input$iRate, 
      &amp;quot;Hours&amp;quot; = input$iHours, 
      &amp;quot;Days&amp;quot; = input$iDays, 
      &amp;quot;Weeks&amp;quot; = input$iWeeks, 
      &amp;quot;Income&amp;quot; = calcIncome())
  })
  
  # Another 2 vectors, this time for min &amp;amp; max.
  # Still wondering why these are not in a dataframe...
  vMin &amp;lt;- as.integer(c(50,6,4,40,55000))
  vMax &amp;lt;- as.integer(c(200,12,7,52,200000))
  
  # OK, now compiling it all into a dataframe. 
  # I swear I&amp;#39;m not sure why I haven&amp;#39;t started with a data frame...
  df &amp;lt;- reactive({data_frame(Param = names(vInput()), 
                             Value = vInput(), 
                             Min = vMin, 
                             Max = vMax, 
                             
                             ## this line below calculates how much you NEED of the current param,
                             ## ...all other things being equal
                             Need = as.integer(input$iIncome/(calcIncome()/vInput())),
                             
                             ## next 4 lines take the real value and scale them to fit into 1 graph
                             ## you don&amp;#39;t want days and hours to be displayed to scale with weeks and...
                             ## ...hundreds of thousands of dollars
                             normNeed = 0,
                             normMin = (vMin - input$iIncome/(calcIncome()/vInput()))/(vMax - vMin),
                             normMax = (vMax - input$iIncome/(calcIncome()/vInput()))/(vMax - vMin),
                             normValue = (vMin - input$iIncome/(calcIncome()/vInput()))/
                                         (vMax - vMin) + 
                                         ((vInput()-vMin))/(vMax - vMin)
  )})
  
  # OK, I added this piece of code recently.
  # I wanted to break out the initial data frame.
  # Otherwise, the total income looked weird on the graph. 
  # It was, like, reversed to the other params
  df2 &amp;lt;- reactive({df() %&amp;gt;% filter(Param != &amp;#39;Income&amp;#39;)})
  df3 &amp;lt;- reactive({df() %&amp;gt;% filter(Param == &amp;#39;Income&amp;#39;)})
  
  
  # Building the plot. Plot1 - plot of all params BUT the annual income
  output$plot1 &amp;lt;- renderPlot({
    ggplot(df2(), aes(x = factor(Param, levels = c(&amp;quot;Weeks&amp;quot;, &amp;quot;Days&amp;quot;, &amp;quot;Hours&amp;quot;, &amp;quot;Rate&amp;quot;)))) +
      
      # Segment between min and max
      # Basically, the grey lines with 2 points at the ends of each line
      geom_segment(aes(xend = Param, y = normMin, yend = normMax), size = 1, col = &amp;quot;grey&amp;quot;)+ 
      geom_point(aes(y = normMax), col = &amp;quot;dark grey&amp;quot;) +
      geom_point(aes(y = normMin), col = &amp;quot;dark grey&amp;quot;) +
      geom_text(aes(y = normMin, label = Min), position = position_nudge(x=-0.1, y=0), size = 4, col = &amp;quot;dark grey&amp;quot;)+
      geom_text(aes(y = normMax, label = Max), position = position_nudge(x=-0.1, y=0), size = 4, col = &amp;quot;dark grey&amp;quot;)+
      
      #The &amp;quot;need&amp;quot;. AKA how much it is supposed to be
      geom_text(aes(y = 0, label = Need), position = position_nudge(x=0.3, y=0.1), size = 6, col = col_need)+
      geom_point(aes(y = normNeed), size = 4, col = col_need) +      
      
      #Your current value
      geom_point(aes(y = normValue), size = 4, col = col_value) +
      geom_text(aes(y = normValue, label = Value), position = position_nudge(x=-0.3), size = 6, col = col_value)+
      
      xlab(&amp;quot;&amp;quot;)+
      ylab(&amp;quot;&amp;quot;)+
      labs(
        title = &amp;#39;Entered and calculated parameters based on other entered parameters&amp;#39;
      ) +
      theme(axis.text.y=element_text(size=14,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;),
            axis.text.x = element_blank(),
            axis.ticks = element_blank()) +
      coord_flip()
  })
  
  #Plot2 - just the annual income.
  #Same stuff, only dots are reversed.
  #Also, $ formatting applied.
  output$plot2 &amp;lt;- renderPlot({
    ggplot(df3(), aes(x = Param)) +
      
      geom_segment(aes(xend = Param, y = normMin, yend = normMax), size = 1, col = &amp;quot;grey&amp;quot;)+ 
      geom_point(aes(y = normMax), col = &amp;quot;dark grey&amp;quot;) +
      geom_point(aes(y = normMin), col = &amp;quot;dark grey&amp;quot;) +
      geom_text(aes(y = normMin, label = paste0(&amp;#39;$&amp;#39;,comma(Min))), position = position_nudge(x=-0.1, y=0), size = 4, col = &amp;quot;dark grey&amp;quot;)+
      geom_text(aes(y = normMax, label = paste0(&amp;#39;$&amp;#39;,comma(Max))), position = position_nudge(x=-0.1, y=0), size = 4, col = &amp;quot;dark grey&amp;quot;)+      
         
      geom_text(aes(y = 0, label = paste0(&amp;#39;$&amp;#39;,comma(Need))), position = position_nudge(x=0.3, y=0.1), size = 6, col = col_value)+
      geom_point(aes(y = normNeed), size = 4, col = col_value) +      
      
      
      geom_point(aes(y = normValue), size = 7, col = &amp;#39;red&amp;#39;) +
      geom_text(aes(y = normValue, label = paste0(&amp;#39;$&amp;#39;,comma(Value))), position = position_nudge(x=-0.3), size = 7, col = &amp;#39;red&amp;#39;)+
      
      xlab(&amp;quot;&amp;quot;)+
      ylab(&amp;quot;&amp;quot;)+
      labs(
        title = &amp;#39;Entered and calculated income based on other entered parameters&amp;#39;
      ) +
      theme(axis.text.y=element_text(size=14,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;),
            axis.text.x = element_blank(),
            axis.ticks = element_blank()) +
      coord_flip()
  })  
  
}

# Et voila!
shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I must say, it is quite nice to be able to embed my Shiny app here on my site which runs on &lt;code&gt;blogdown&lt;/code&gt;: my original Medium post wasn’t able to do that&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On Importance of Minimalism in Retrospective Analytics</title>
      <link>/2018/01/12/sass/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/12/sass/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/on-importance-of-minimalism-in-retrospective-analytics-75c5a02c2c83&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/pulse/importance-minimalism-retrospective-analytics-taras-kaduk-1/&#34;&gt;&lt;strong&gt;LinkedIn&lt;/strong&gt;&lt;/a&gt; &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hey, analyst, how is life? Talk to me. Do you love what you do for life, do you like all things data?&lt;/p&gt;
&lt;p&gt;Yet, do you sometimes feel like you’re a Sisyphus rolling a giant rock of data up the hill every day, only to see it go down with a racket in the evening (and you know what you’re going to do tomorrow)? Or, do you imagine yourself being a plate spinner at a circus, only instead of plates and poles you’ve got five dozen reports to spin, and instead of an entertained crowd you’ve got your co-workers, managers and senior executives watching your “performance” and asking to add more plates, and God forbid any one plate falls?&lt;/p&gt;
&lt;p&gt;The truth is — herding your reporting under one roof is no small feat. It is especially challenging at times, when you need to create a unified business solution from bits and pieces: Excel spreadsheets, Access databases, and built-in reports older than some of your employees. Create it and then maintain it. Oh my.&lt;/p&gt;
&lt;p&gt;It doesn’t help that such enormous task usually receives minimal funding, as you are basically “fixing” something that “ain’t broke”.&lt;/p&gt;
&lt;p&gt;It’s OK. I’ve been there. Coming to the office all excited to work with data and create some mad data viz and publish it, only to get a hard kick in a sensitive area with one report not refreshing, another report having errors, and with a dozen more requests to spin off more of the same.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;the-sass-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The SASS Framework&lt;/h1&gt;
&lt;p&gt;But it doesn’t have to be this way. Over the years, my team and I have managed to develop a simple framework for dealing with this kind of plate-spinning environment. We are a very lean team, but we manage to accomplish a lot, with minimum time spent fixing what’s broken and/or doing anything that resembles what’s been done before.&lt;/p&gt;
&lt;p&gt;The secret sauce here is optimizing for time. You don’t have the capacity to create isolated one-time-only solutions, to fix things on a regular basis, to perform manual updates, to enter data, you’ve got time for none of that. Much like professional cyclists reduce the weight of a bicycle by replacing parts, trimming of edges of nuts and bolts and drilling holes through bike pipes, you need to save as much time as possible by shaving off all the nonessential movements.&lt;/p&gt;
&lt;p&gt;I call our result approach the SASS framework. SASS here stands for simple, automated, standard and scalable. Building your work around these four “standards” would help you get out of the self-perpetuated loop of constant plate-spinning.&lt;/p&gt;
&lt;p&gt;Applying this framework is simple. You ask these 4 questions on simplicity, automation, standard and scale. Furthermore, they are applicable on every stage of you data’s journey: whether you only take on a new request, build out a report, or deploy it for continuous consumption.&lt;/p&gt;
&lt;p&gt;Below, I’ll show you some examples of what it means.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;simple&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/sass/simple.png&#34; alt=&#34;Simple is good. It’s that simple (pun intended).&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Simple is good. It’s that simple (pun intended).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Do you reports suffer from abundance of KPIs, complicated data models, transformation steps no one understands? Take a step back and simplify it. Keep your data tidy and your calculations simple. Your code should be laconic, elegant and easy to read. For me, complicated code is the first sign of a possible trouble. Do you have to many visuals? More opportunities of something failing. Reduce. Do have multiple reports running doing a similar thing? Reduce the amount and consolidate them. Keep it simple.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automated&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automated&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/sass/automated.png&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;“That which does not kill us, makes us stronger.” &lt;strong&gt;~Nietzsche&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well… then redundant work definitely doesn’t make me stronger. It slowly kills me. And it kills me seeing other people doing it.&lt;/p&gt;
&lt;p&gt;A wise man once said: “Life is too short to do what can be automated”. He also said: “Automate until it hurts”. (Sadly, he wasn’t heard and his wisdom almost vanished. But then he decided to write this blog post)&lt;/p&gt;
&lt;p&gt;Do your data sources refresh automatically? Do you script all of your transformations (be it SQL, R, Python, Power Query, or a screen grab script), or do you perform any actions manually (save a file, format a spreadsheet, paste a table etc)? Is your solution on a scheduled refresh, or do you have click on things?&lt;/p&gt;
&lt;p&gt;It is tempting to let some of the manual action slip in as “no big deal”. That’s fine and dandy when you have 5–10 reports to maintain. Scale it up to 50 — and your “no big deal” adds up to staying late and spending weekends running shit you’re not supposed to.&lt;/p&gt;
&lt;p&gt;You’re too valuable to do some script’s dirty job.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standard&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/sass/standard.gif&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This one must be simple and obvious. Keeping things standard again means more time savings: less validation, less discovery time, less training and explaining.&lt;/p&gt;
&lt;p&gt;Do you use one record of truth, the data from one lake? If half your analysts use one source (say, a new database on the cloud) and another half is running stored procedures written before the times of Noah’s Ark, then don’t get surprised if the two teams end up with a different number.&lt;/p&gt;
&lt;p&gt;Do you and/or your team adhere to the same visual design principles? Nothing beats bars and lines, and nothing beats simple colors. Most of our recurring reporting looks pretty plain and boring: it is bars, lines and tables; and the color is designated to carry a specific data point and almost never used just for the looks. Yeah, it isn’t fun, but you avoid the decision fatigue, you save time by not coming up with a new palette and new outline, and you don’t spend time explaining your audience what this new spider web chart means and how to read it. Don’t get me wrong, I’m not against new reports, cool colors, interesting and fresh charts — I’m just showing you where to save time and resources when you need to maintain a large number of such reports.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scalable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scalable&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/sass/scalable.gif&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;After all said and and done, you should be able to repurpose your logic for something else. Or scale it within the given context. Similarly, many solutions can be created on top of some previous work, instead of creating a new solution. This also makes sure your solutions are “standardized”: you kill 2 birds with 1 stone.&lt;/p&gt;
&lt;p&gt;In our line of work, we’ve created several large data models for financial, sales and operational data, and most of the existing and new reports feed off these data models. We save a lot of time deploying new solutions because most of the work is already done and the data models have been built in such a way that we easily can scale it for something new.&lt;/p&gt;
&lt;p&gt;Of course, this minimalist approach in retrospective analytics is not for everyone. There are different roles in the world of data science, and different tactics work for each role. The SASS framework is mostly applicable to the teams or individuals responsible for running a large suite of recurring reporting within a mid to large size company.&lt;/p&gt;
&lt;p&gt;Let me know if you’re facing similar struggles at work and if you have different solutions in place!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>When your data analysis is validated on The Late Show with Stephen Colbert</title>
      <link>/2018/01/10/mpaa-damon/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/10/mpaa-damon/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/when-your-data-analysis-is-validated-on-the-late-show-with-stephen-colbert-b7cd6ca37147&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;About a month ago, I wrote a little article about the MPAA rating system. I set up to find out if their lettering system does any justice to the actual content seen on the screen. Briefly speaking, it does, but with caveats.&lt;/p&gt;
&lt;p&gt;One of such caveats was the effect of profanity. What my quick and dirty data analysis showed was that profanity was the sure thing that could send a movie into an R category:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/profanity-1.png&#34; style=&#34;width:100.0%&#34; /&gt; &lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/profanity2-1.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I even wrote this in my &lt;a href=&#34;/post/mpaa/mpaa/#quote&#34;&gt;blog post&lt;/a&gt;, when summing up the effects of profanity on movie’s rating:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Good Will Hunting is neither violent nor sexually explicit, but it is profane AF, and, sure enough, is R rated for - wait for it - &lt;em&gt;“strong language, including some sex-related dialogue”.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;when-matt-damon-proves-you-right&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;When Matt Damon proves you right&lt;/h2&gt;
&lt;p&gt;So, a few days ago we were watching &lt;strong&gt;The Late Show with Stephen Colbert&lt;/strong&gt;, and this bit with Matt Damon caught my instant attention:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I remember when people at MIRAMAX came to us and said &lt;em&gt;“Could you make it [Good Will Hunting] &lt;strong&gt;PG-13?&lt;/strong&gt;”&lt;/em&gt; There’s no violence or sex to speak of, it’s just…&lt;/p&gt;
&lt;p&gt;And I said &lt;em&gt;“What’s making it rated &lt;strong&gt;R&lt;/strong&gt;?”&lt;/em&gt;, and they said &lt;em&gt;“the language”&lt;/em&gt;, and I said &lt;em&gt;“Okay well so we could loop a couple lines”&lt;/em&gt;, and they go &lt;em&gt;“Yeah but you’re only allowed”&lt;/em&gt;… I think at the time you were allowed to say the &lt;strong&gt;F-word&lt;/strong&gt; three times, and I said &lt;em&gt;“Okay, well how many are we off by?”&lt;/em&gt; And they said &lt;em&gt;“You go over by a hundred and forty-five”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/2GrKY7Qqal8?start=220&amp;amp;end=305&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;Ha! So, my theory checks out! It’s profanity that makes a movie R rated! It can be puritan and pacifistic, but you drop a couple of F-bombs — and you’re out.&lt;/p&gt;
&lt;p&gt;It is funny that I chose exactly Good Will Hunting as an example of how an otherwise modest movie can be sent straight to the R bench for what Matt claims is how they all talk in Boston.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/good_will_hunting-1.png&#34; /&gt;

&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;Well, anyway, that’s it folks. Do more data analysis, for work and for fun.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Create World Pixel Maps in R</title>
      <link>/2017/11/26/pixel-maps/</link>
      <pubDate>Sun, 26 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/26/pixel-maps/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/r-walkthrough-create-a-pixel-map-537ce12c2f0c&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Today, I’m going to show you how to make pixel maps in R. Why pixel maps? Because they look awesome!&lt;/p&gt;
&lt;p&gt;I was searching on the web for a while, but couldn’t find a good tutorial. Being stubborn as I am, I eventually figured out a way to get what I want. You know, if you torture your code enough, it might give you what you need.&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;First, of course, loading required packages. These days, I don’t bother with discrete packages and get the entire &lt;code&gt;tidyverse&lt;/code&gt; right away. Aside from that, you may need the &lt;code&gt;ggmap&lt;/code&gt; package, which I used in the earlier iterations of this script (more on that later). You’ll also need the &lt;code&gt;maps&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Library -----------------------------------------------------------------

library(tidyverse)
library(googlesheets)
library(maps)
library(here)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll need our data points. You can do anything you want here: load a google sheet with data, reach to you Google Maps data, import a csv file, whatever your heart desires.&lt;/p&gt;
&lt;p&gt;Initially, I created a data frame with places I’ve been to, and then grabbed their coordinates with &lt;code&gt;mutate_geocode()&lt;/code&gt; function from a &lt;code&gt;ggmap&lt;/code&gt; package. That piece of code takes a while to run, and the list doesn’t really change that much, so I ended up saving it as a separate Google sheet, and now I simply import it. But you do as you wish.&lt;/p&gt;
&lt;p&gt;You’ll obviously need to replace this chunk with your own data. I include &lt;code&gt;tail&lt;/code&gt; of my tibble to give you an idea about the data structure&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get data -------------------------------------------------------------
url &amp;lt;- &amp;#39;https://docs.google.com/spreadsheets/d/e/2PACX-1vQoRxmeOvIAQSqOtr2DMOBW_P4idYKzRmVtT7lpqwoH7ZWAonRwOcKR2GqE-yqUOhb5Ac_RUs4MBICe/pub?output=csv&amp;#39;
destfile &amp;lt;- &amp;quot;locations.csv&amp;quot;
curl::curl_download(url, destfile)
locations &amp;lt;- read_csv(destfile)
tail(locations)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   city                    status   lon   lat family
##   &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;
## 1 Hurgada, Egypt          been    33.8  27.3      2
## 2 Simferopol, Ukraine     been    34.1  45.0      2
## 3 Yalta, Ukraine          been    34.2  44.5      2
## 4 Dnipropetrivsk, Ukraine been    35.0  48.5      2
## 5 Zaporizhya, Ukraine     been    35.1  47.8      1
## 6 Moscow, Russia          been    37.6  55.8      1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rounding-the-coordinates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rounding the coordinates&lt;/h2&gt;
&lt;p&gt;As I’m creating a pixel map - I need dots in the right places. I’m going to plot a dot for each degree, and therefore I need my coordinates rounded to the nearest degree&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;locations &amp;lt;- locations %&amp;gt;% 
        mutate(long_round = round(lon, 0),
               lat_round = round(lat,0))
tail(locations)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
##   city                    status   lon   lat family long_round lat_round
##   &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Hurgada, Egypt          been    33.8  27.3      2        34.       27.
## 2 Simferopol, Ukraine     been    34.1  45.0      2        34.       45.
## 3 Yalta, Ukraine          been    34.2  44.5      2        34.       44.
## 4 Dnipropetrivsk, Ukraine been    35.0  48.5      2        35.       48.
## 5 Zaporizhya, Ukraine     been    35.1  47.8      1        35.       48.
## 6 Moscow, Russia          been    37.6  55.8      1        38.       56.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;generate-a-pixel-grid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generate a pixel grid&lt;/h2&gt;
&lt;p&gt;The next step is the key to getting a pixel map. We’ll fill the entire plot with a grid of dots - 180 dots from south to north, and 360 dots from east to west, but then only keep the dots that are on land. Simple!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Generate a data frame with all dots -----------------------------------------------

lat &amp;lt;- data_frame(lat = seq(-90, 90, by = 1))
long &amp;lt;- data_frame(long = seq(-180, 180, by = 1))
dots &amp;lt;- lat %&amp;gt;% 
        merge(long, all = TRUE)

## Only include dots that are within borders. Also, exclude lakes.
dots &amp;lt;- dots %&amp;gt;% 
        mutate(country = map.where(&amp;#39;world&amp;#39;, long, lat),
               lakes = map.where(&amp;#39;lakes&amp;#39;, long, lat)) %&amp;gt;% 
        filter(!is.na(country) &amp;amp; is.na(lakes)) %&amp;gt;% 
        select(-lakes)

head(dots)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   lat long                   country
## 1 -83 -173                Antarctica
## 2 -83 -172                Antarctica
## 3 -83 -171                Antarctica
## 4  60 -167 USA:Alaska:Nunivak Island
## 5  60 -166 USA:Alaska:Nunivak Island
## 6  65 -166                USA:Alaska&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;And now the easy part. Plotting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note that this post, just like this entire site, runs on &lt;code&gt;blogdown&lt;/code&gt;, and the post is created via Rmarkdown. When the plots render here - they look ugly-ish due to the fact that geom_point doesn’t scale down along with the plot. The output on your machine will look better. Take a look at the head image to understand how your output may look like&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme &amp;lt;- theme_void() +
        theme(panel.background = element_rect(fill=&amp;quot;#212121&amp;quot;),
              plot.background = element_rect(fill=&amp;quot;#212121&amp;quot;),
              plot.title=element_text(face=&amp;quot;bold&amp;quot;, colour=&amp;quot;#3C3C3C&amp;quot;,size=16),
              plot.subtitle=element_text(colour=&amp;quot;#3C3C3C&amp;quot;,size=12),
              plot.caption = element_text(colour=&amp;quot;#3C3C3C&amp;quot;,size=10),  
              plot.margin = unit(c(0, 0, 0, 0), &amp;quot;cm&amp;quot;))

plot &amp;lt;- ggplot() +   
        #base layer of map dots
        geom_point(data = dots, aes(x=long, y = lat), col = &amp;quot;grey45&amp;quot;, size = 0.7) + 
        #plot all the places I&amp;#39;ve been to
        geom_point(data = locations, aes(x=long_round, y=lat_round), color=&amp;quot;grey80&amp;quot;, size=0.8) + 
        #plot all the places I lived in, using red
        geom_point(data = locations %&amp;gt;% filter(status == &amp;#39;lived&amp;#39;), aes(x=long_round, y=lat_round), color=&amp;quot;red&amp;quot;, size=0.8) +
        #an extra layer of halo around the places I lived in
        geom_point(data = locations %&amp;gt;% filter(status == &amp;#39;lived&amp;#39;), aes(x=long_round, y=lat_round), color=&amp;quot;red&amp;quot;, size=6, alpha = 0.4) +
        #adding my theme
        theme
plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/pixel-maps/pixel-maps_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You probably want to save the map, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;map_full.jpg&amp;#39;, 
       device = &amp;#39;jpg&amp;#39;, 
       path = getwd(), 
       width = 360, 
       height = 180, 
       units = &amp;#39;mm&amp;#39;,
       dpi = 250)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the map of the entire world can be overwhelming and sad, especially if you, just like me, are not much of a traveler. Look at it! There aren’t many dots! WTF?! Sad!&lt;/p&gt;
&lt;p&gt;You can zoom in on an area you did cover (e.g. include USA only), either computationally (calculate you westernmost, easternmost, southernmost and northernmost points and pass them as xlim and ylim), or excluding continents from the map with &lt;code&gt;dplyr&lt;/code&gt; (excluding Antarctica at least would be a good idea). You can also use a different map to start with - World map may not be necessary for some tasks. I used it because I was fortunate enough to live on 2 continents, but your mileage may vary.&lt;/p&gt;
&lt;p&gt;In all of these cases, you may want to reconsider the grain of the map: if you zoom in on USA only, you may want to choose to plot a dot for every 0.5 degrees, and then would need to adjust your coordinate rounding respectively (round to the nearest half of degree). Why do it? The finer your grain - the more details you’ll get. For instance, with a grain of 1 degree, San Francisco, San Mateo, San Rafael and Oakland are all be one same dot.&lt;/p&gt;
&lt;p&gt;I could definitely program my way though this scaling issue and create a parameter, and make other variables depend on it… I don’t find this exercise to be particularly useful in this case. If you get it done - awesome!&lt;/p&gt;
&lt;p&gt;For my case, I wanted a wide banner, so I chose some specific arbitrary limits that looked good to me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot + scale_y_continuous(limits = c(10, 70), expand = c(0,0)) +
        scale_x_continuous(limits = c(-150,90), expand = c(0,0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/pixel-maps/pixel-maps_files/figure-html/map-wide-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outro&lt;/h2&gt;
&lt;p&gt;Obviously, there is so much more to do with this. The possibilities are endless. The basic idea is pretty simple - generate a point grid and plot rounded coordinates on top of the grid.&lt;/p&gt;
&lt;p&gt;Let me know if you find new implementations of this code!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;repo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Repo&lt;/h2&gt;
&lt;p&gt;As this blog is rendered with &lt;code&gt;blogdown&lt;/code&gt;, all the source code is on Github for your pleasure. taraskaduk.com repo is at &lt;a href=&#34;https://github.com/taraskaduk/taraskaduk/&#34; class=&#34;uri&#34;&gt;https://github.com/taraskaduk/taraskaduk/&lt;/a&gt; and the Rmd file for this post should be located at &lt;a href=&#34;https://github.com/taraskaduk/taraskaduk/tree/master/content/post&#34; class=&#34;uri&#34;&gt;https://github.com/taraskaduk/taraskaduk/tree/master/content/post&lt;/a&gt; (unless I mess it all up and relocate it. I’m really not good at your whole github and blogdown thing. But I’m learning)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Do MPAA movie ratings mean anything?</title>
      <link>/2017/09/30/mpaa/</link>
      <pubDate>Sat, 30 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/30/mpaa/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/do-mpaa-movie-ratings-mean-anything-1d1ab683f21d&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt; &lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;Being a parent in modern days is lots of fun. Not only all of us are pretty much winging it, not having any idea what we’re doing &lt;em&gt;(seriously, you need a license to do braids and nails, yet raising a human being a future member of society is a no-brainer, right?)&lt;/em&gt;  — we are also constantly being watched and judged by other parents.&lt;/p&gt;
&lt;p&gt;When it comes to watching movies with our six-year-old son, we don’t have a strict set of rules. We pretty much fly by the seat of our pants with &lt;em&gt;“I know it when I see it”&lt;/em&gt; approach to violence, profanity, or any other content. Not to say that we’re watching Pulp Fiction and Basic Instinct (the most challenging movie to date was probably Alice in the Wonderland), but all the movies we watch with our son are between G and PG - and we hardly can tell a difference between the two.&lt;/p&gt;
&lt;p&gt;That’s why I was surprised to find out that some parents swear by this MPAA rating system, and use it religiously when deciding what their kids can and can’t watch.&lt;/p&gt;
&lt;p&gt;And it’d be all good if I haven’t noticed that these ratings are sometimes kind of… &lt;em&gt;arbitrary?&lt;/em&gt; So, I decided to dig into the data. Because data will solve all of our problems, right?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;I searched around a bit, and stumbled upon a few awesome things. First, apparently &lt;a href=&#34;http://www.imdb.com/&#34;&gt;imdb.com&lt;/a&gt; has a parental section for every movie. However, these guides are not standard in the way they are filled out, and scrubbing IMDb for this data wouldn’t get me where I wanted to be fast enough. And then I stumbled upon this awesome website called &lt;a href=&#34;http://kids-in-mind.com&#34;&gt;kids-in-mind.com&lt;/a&gt;. It had a lot of info similar or equal to one contained on &lt;a href=&#34;http://www.imdb.com/&#34;&gt;IMBd.com&lt;/a&gt;, but it had a crucial key component: every movie on this website is rated on an 11-point scale, from 0 to 10, on three metrics: &lt;strong&gt;sex &amp;amp; nudity&lt;/strong&gt;, &lt;strong&gt;violence &amp;amp; gore&lt;/strong&gt;, and &lt;strong&gt;profanity&lt;/strong&gt;. Well, this is just perfect! Not only that — it also has that MPAA rating data point for every movie, which means I get all of my data in one sitting.&lt;/p&gt;
&lt;p&gt;So, I wrote a little R script using &lt;code&gt;rvest&lt;/code&gt; package, and got my data into a tidy data frame, and started exploring. After a little bit of data wrangling (I excluded &lt;strong&gt;NR&lt;/strong&gt; movies as they are obviously &lt;em&gt;not rated&lt;/em&gt;, and are all over the place. Also, Kids In Mind database didn’t have many &lt;strong&gt;NC-17&lt;/strong&gt; rated movies, therefore I combined them with &lt;strong&gt;R&lt;/strong&gt; rated films), I got my first results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library() -------------------------------------------------------------------

library(readr)
library(stringr)
library(tidyverse)
library(rvest)
library(ggrepel)

# Extract -----------------------------------------------------------------
## Here, I decided to be easy on the website and not to scrape it every time: instead, I saved the output and only let the web scraping run is the output isn&amp;#39;t found.

path &amp;lt;- getwd()

if (file.exists(paste(path,&amp;#39;movies_extract.csv&amp;#39;, sep = &amp;quot;/&amp;quot;))) {
  movies_df &amp;lt;- read_csv(paste(path,&amp;#39;movies_extract.csv&amp;#39;, sep = &amp;quot;/&amp;quot;))
} else {
        
        
## This is my first attempt ever to use rvest and also the first time I work with regex. Obviously, the code need improvement! Comments are welcome!

        
## The site has data displayed in a table, broken down into pages, and the url contains the top movie&amp;#39;s position and a range of 3 metrics - sex, violence and profanity - from 0 to 10.
## The way the data comes out after scraping, I had a problem with movies rated 10 on profanity, as didn&amp;#39;t know how to let my regex know that 10 is 10 (it took all 10 as 1, and appended 0 to the next movie in line)
## I couldn&amp;#39;t figure it out!!! So, I made a loop inside the loop. That&amp;#39;s what j and k variables are for.
        
  movies_vector &amp;lt;- character()
  j&amp;lt;-10
  k&amp;lt;-0
  i&amp;lt;-0
 
  for(j in 10:9) {
    if (j == 9) k &amp;lt;- 0 else k &amp;lt;- 10
    position &amp;lt;- 0
    
    for(i in 0:300){
      url &amp;lt;- paste(&amp;quot;http://www.kids-in-mind.com/cgi-bin/listbyrating/search.pl?query=&amp;amp;stpos=&amp;quot;, 
                   position, 
                   &amp;quot;&amp;amp;stype=AND&amp;amp;s1=0&amp;amp;s2=10&amp;amp;v1=0&amp;amp;v2=10&amp;amp;p1=&amp;quot;,
                   k,
                   &amp;quot;&amp;amp;p2=&amp;quot;,
                   j,
                   &amp;quot;&amp;amp;m=1&amp;amp;m=2&amp;amp;m=3&amp;amp;m=4&amp;quot;, sep = &amp;quot;&amp;quot;)
      
      import &amp;lt;- read_html(url)
      
      vector &amp;lt;- import %&amp;gt;%
        html_node(&amp;quot;.t11normal+ p&amp;quot;) %&amp;gt;%
        html_text() %&amp;gt;% 
        str_replace_all(&amp;#39;\\\n\\\n&amp;#39;, &amp;#39;&amp;#39;) %&amp;gt;% 
        str_replace_all(&amp;#39;\\[\\[&amp;#39;, &amp;#39;[&amp;#39;) %&amp;gt;%
        str_replace_all(&amp;#39;\\]\\]&amp;#39;, &amp;#39;]&amp;#39;)
## This is that pesky regex. If the movie is return on the run that ask for movies rated 0 to 9 on profanity, then we know it is 0-9 as the last digit...      
      if(j == 9){
        vector &amp;lt;- vector %&amp;gt;% 
          str_extract_all(&amp;#39;([^\\]]* \\[\\d{4}\\] \\[\\S+\\] - [0-9]{1,2}.[0-9]{1,2}.[0-9])&amp;#39;)

      ##...otherwise, we know the last number is a 10!
      } else {
        vector &amp;lt;- vector %&amp;gt;% 
          str_extract_all(&amp;#39;([^\\]]* \\[\\d{4}\\] \\[\\S+\\] - [0-9]{1,2}.[0-9]{1,2}.10)&amp;#39;)
      }
      movies_vector &amp;lt;- c(movies_vector,vector[[1]])
      
      position &amp;lt;- position + 34
      if(any(duplicated(movies_vector)) == TRUE) break
    }

## Also, I couldn&amp;#39;t figure out a good way to stop the loop. If you keep running it after the last movie, you&amp;#39;ll return to the start and begin again. So, as dumb as it is, I simply looked up the last movie in the list and wrote it down. Given that Zootopia starts with a Z and followed by an O, it is unlikely there will be many more movies after this. I&amp;#39;ll take my chances.
    
    if(any(grepl(&amp;#39;Zootopia&amp;#39;,movies_vector)) == TRUE) break
  }
  
 
  ## Some stringr FTW
  movies_df &amp;lt;- 
    as_data_frame(movies_vector) %&amp;gt;% 
    separate(value, sep = &amp;#39;\\[&amp;#39;, remove = TRUE, into = c(&amp;#39;name&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;rating&amp;#39;)) %&amp;gt;% 
    separate(rating, sep = &amp;#39;\\] - &amp;#39;, remove = TRUE, into = c(&amp;#39;mpaa&amp;#39;, &amp;#39;rating&amp;#39;)) %&amp;gt;% 
    separate(rating, sep = &amp;#39;\\.&amp;#39;, remove = TRUE, into = c(&amp;#39;sex&amp;#39;, &amp;#39;violence&amp;#39;, &amp;#39;profanity&amp;#39;))
 
  ## Some dplyr FTW
  movies_df &amp;lt;- movies_df %&amp;gt;% 
    mutate(
      year = str_replace(movies_df$year, &amp;#39;\\]&amp;#39;, &amp;#39;&amp;#39;),
      sex = as.numeric(sex),
      violence = as.numeric(violence),
      profanity = as.numeric(profanity),
      mpaa = factor(mpaa, levels = c(&amp;#39;G&amp;#39;, &amp;#39;PG&amp;#39;, &amp;#39;PG-13&amp;#39;, &amp;#39;R&amp;#39;, &amp;#39;NC-17&amp;#39;, &amp;#39;NR&amp;#39;))
    ) 
  
  ##Save output - I didn&amp;#39;t want to ping their website every time. 
  write_csv(movies_df, paste(path,&amp;#39;movies_extract.csv&amp;#39;, sep = &amp;quot;/&amp;quot;))
  remove(i,j,k, movies_vector, position, url, vector, import)
}

caption &amp;lt;- &amp;#39;\ntaraskaduk.com\nbased on data from kids-in-mind.com&amp;#39;



# Transform ---------------------------------------------------------------

movies_df &amp;lt;- movies_df %&amp;gt;% 
  filter(mpaa != &amp;#39;NR&amp;#39;) %&amp;gt;% ## Not Rated movies are NOT RATED
  filter(name != &amp;#39;Mozart\&amp;#39;s Sister&amp;#39;) %&amp;gt;%  ## This one is the one I caught that is wrong - it is actually NR and not G.
  mutate(avg = (sex + profanity + violence)/3)

## Not enough NC-17 movies - let&amp;#39;s match them up with R
movies_df$mpaa &amp;lt;- movies_df$mpaa %&amp;gt;% recode(R = &amp;#39;R &amp;amp; NC-17&amp;#39;, `NC-17` = &amp;#39;R &amp;amp; NC-17&amp;#39;) 

movies_df$mpaa &amp;lt;- factor(movies_df$mpaa, levels = c(&amp;#39;G&amp;#39;, &amp;#39;PG&amp;#39;, &amp;#39;PG-13&amp;#39;, &amp;#39;R &amp;amp; NC-17&amp;#39;))

movies_gather &amp;lt;- movies_df %&amp;gt;% 
  gather(key = metric, value = score, c(sex, violence, profanity, avg))


# Graphs -----------------------------------------------------------------

theme &amp;lt;- theme(
  legend.position=&amp;quot;none&amp;quot;,
  axis.ticks.y=element_blank(),
  panel.grid.major.y = element_line(colour=&amp;quot;#e0e0e0&amp;quot;,size=40),
  panel.grid.major.x =element_line(colour=&amp;quot;#F0F0F0&amp;quot;,size=.75),
  panel.grid.minor =element_blank(),
  panel.background=element_rect(fill=&amp;quot;#F0F0F0&amp;quot;),
  plot.background=element_rect(fill=&amp;quot;#F0F0F0&amp;quot;),
  plot.title=element_text(face=&amp;quot;bold&amp;quot;, colour=&amp;quot;#3C3C3C&amp;quot;,size=16),
  plot.subtitle=element_text(colour=&amp;quot;#3C3C3C&amp;quot;,size=12),
  plot.caption = element_text(colour=&amp;quot;#3C3C3C&amp;quot;,size=10),  
  axis.text.x=element_text(size=11,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;),
  axis.text.y=element_text(size=11,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;),
  axis.title.y=element_text(size=11,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;,vjust=1.5),
  axis.title.x=element_text(size=11,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;,vjust=-0.5),
  plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), &amp;quot;cm&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;on-average-higher-mpaa-rating-follows-higher-levels-of-inappropriate-content-but&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;On average, higher MPAA rating follows higher levels of inappropriate content, but…&lt;/h3&gt;
&lt;p&gt;The first result seemed fairly obvious: higher (stricter) MPAA ratings have a higher rate of violence, sex and profanity. &lt;strong&gt;On average&lt;/strong&gt;. However, the amount of overlap is astonishing. Basically, any category is entirely consumed by its two neighboring categories.&lt;/p&gt;
&lt;p&gt;What’s more, you can always find a movie in a “lower” category that is more inappropriate than some other movie in a “higher” category: Jimmy Neutron VS Little Rascals, the 5th Harry Potter VS Life is Beautiful, Year One VS The King’s Speech etc.&lt;/p&gt;
&lt;p&gt;You can see this from the figure below. You may also notice that there movies scoring 2.5 points on average that are in every MPAA category. We’ll come back to this later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels_all &amp;lt;- movies_df %&amp;gt;% 
  filter(name == &amp;quot;Jimmy Neutron: Boy Genius&amp;quot; | 
           name == &amp;#39;Adventures of Elmo In Grouchland, The&amp;#39; |
           name == &amp;quot;Harry Potter and the Half-Blood Prince&amp;quot; |
           name == &amp;#39;Year One&amp;#39; |
           name == &amp;#39;Halloween&amp;#39; |
           name == &amp;#39;Little Rascals, The&amp;#39; |
           name ==  &amp;#39;Life is Beautiful&amp;#39; |
           name == &amp;#39;King\&amp;#39;s Speech, The&amp;#39;
  )

ggplot(data = movies_df, aes(x=avg, y = mpaa, col = mpaa)) +
  geom_jitter(alpha = 0.2) +
  scale_colour_brewer(palette = &amp;quot;Spectral&amp;quot;, direction = -1) +
  geom_point(data = labels_all, size = 3) +
  geom_label_repel(
    data = labels_all,
    aes(label = name),
    size = 3,
    nudge_y = 0.1) +
  ylab(&amp;#39;MPAA Rating&amp;#39;) + xlab(&amp;#39;Average (violence &amp;amp; gore, sex, profanity)&amp;#39;) +
  labs(
    title = &amp;#39;MPAA rating isn\&amp;#39;t everything&amp;#39;,
    subtitle = &amp;#39;Visualizing the amount of overlap between categories&amp;#39;,
    caption = caption) +
  theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/mpaa-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mpaa-is-most-forgiving-on-violence&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MPAA is most forgiving on violence&lt;/h3&gt;
&lt;p&gt;Well, no kidding! This was hardly a surprise. As a foreigner, I am constantly amused by how much violence is considered appropriate, contrasted with, for example, how little nudity is acceptable. &lt;em&gt;Guts and blood? Body parts?&lt;/em&gt; &lt;strong&gt;Sure, bring it on!&lt;/strong&gt; &lt;em&gt;Naked breasts?&lt;/em&gt; &lt;strong&gt;How dare you!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So, next time you rent a G rated movie and think it is clean - think again. It’s probably just as violent as that other PG movie you wanted. Both G and PG movies center around 3 points on violence anyway, with max points being 5 for G and 6 for PG. Just go with PG then, eh?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels_g_pg &amp;lt;- movies_df %&amp;gt;% 
  filter(name == &amp;quot;Babe: Pig in the City&amp;quot; | 
           name == &amp;quot;Beauty and the Beast&amp;quot; &amp;amp; mpaa == &amp;#39;G&amp;#39; |
           name == &amp;#39;Zeus and Roxanne&amp;#39; |
           name == &amp;#39;Sleepless in Seattle&amp;#39;)

ggplot(data = movies_df %&amp;gt;% filter(mpaa %in% c(&amp;#39;PG&amp;#39;, &amp;#39;G&amp;#39;)), aes(x=as.factor(violence), y = mpaa, col = as.factor(violence))) +
  geom_jitter(alpha = 0.5, size =3) +
  scale_colour_brewer(palette = &amp;quot;Spectral&amp;quot;, direction = -1) +
  geom_point(data = labels_g_pg, size = 5) +
  geom_label_repel(
    data = labels_g_pg,
    aes(label = name),
    size = 4,
    col = &amp;#39;grey51&amp;#39;,
    nudge_x = 1,
    nudge_y = 0) +
  theme +
  theme(axis.text.y = element_text(size = rel(1.2)),
        panel.grid.major.y = element_line(colour=&amp;quot;#e0e0e0&amp;quot;,size=70)) +
  ylab(&amp;#39;MPAA Rating&amp;#39;) + xlab(&amp;#39;Violence and Gore&amp;#39;) +
  labs(
    title = &amp;#39;That G movie you felt safe about&amp;#39;,
    subtitle = &amp;#39;is probably just as violent as the PG one you rejected&amp;#39;,
    caption = caption)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/violence-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-the-is-up-with-profanity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What the **** is up with profanity?&lt;/h3&gt;
&lt;p&gt;Now, this is a zero tolerance zone in the movie world. Not sex and nudity, as I assumed. Profanity. Unlike other categories, where scores flow gradually from category to category, profanity has some clear trends:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All G movies are bundled up in a narrow 0-2 points corridor&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Most PG-13 movies are between 4 and 5 points on profanity&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;R and NC-17 movies reside between 5 and 10 points&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I bet if I was trying to predict an MPAA rating based on these criteria, profanity would be the strongest predictor (not a concern of this post, but maybe later)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels_profanity &amp;lt;- movies_df %&amp;gt;% 
  filter(name %in% c(&amp;#39;Aladdin&amp;#39;,
                &amp;#39;Beethoven&amp;#39;,
                &amp;#39;Life is Beautiful&amp;#39;,
                &amp;#39;Psycho&amp;#39;,
                &amp;#39;Cars&amp;#39;,
                &amp;#39;Apollo 13&amp;#39;,
                &amp;#39;Nutty Professor, The&amp;#39;,
                &amp;#39;Straight Outta Compton&amp;#39;)
         )

ggplot(data = movies_df, aes(x=as.factor(profanity), y = mpaa, col = mpaa)) +
  geom_jitter(alpha = 0.2) +
  scale_colour_brewer(palette = &amp;quot;Spectral&amp;quot;, direction = -1) +
  geom_point(data = labels_profanity, size = 3) +
  geom_label_repel(
    data = labels_profanity,
    aes(label = name),
    size = 3,
    nudge_y = 0.1) +
  ylab(&amp;#39;MPAA Rating&amp;#39;) + xlab(&amp;#39;Profanity&amp;#39;) +
  labs(
    title = &amp;#39;Profanity rating patterns across MPAA categories&amp;#39;,
    caption = caption) +
  theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/profanity-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at R &amp;amp; NC-17 section, it is tempting to dive in a bit more. Let’s go!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels_r &amp;lt;- movies_gather %&amp;gt;% 
  filter(metric != &amp;#39;avg&amp;#39;) %&amp;gt;% 
  filter(metric == &amp;#39;violence&amp;#39; &amp;amp; 
           name %in% c(
             &amp;#39;Sex and the City&amp;#39;, 
             &amp;#39;Basic Instinct&amp;#39;, 
             &amp;#39;Saw&amp;#39;, 
             &amp;#39;Nightmare on Elm Street, A&amp;#39;) | 
           metric == &amp;#39;sex&amp;#39; &amp;amp; 
           name %in% c(
             &amp;#39;Reservoir Dogs&amp;#39;, 
             &amp;#39;Love Actually&amp;#39;, 
             &amp;#39;Basic Instinct&amp;#39;, 
             &amp;#39;American Pie&amp;#39;) |
           metric == &amp;#39;profanity&amp;#39; &amp;amp; 
           name %in% c(
             &amp;#39;Psycho&amp;#39;,  
             &amp;#39;Office Space&amp;#39;,
             &amp;#39;Pulp Fiction&amp;#39;,
             &amp;#39;Old School&amp;#39;,
             &amp;#39;Anchorman&amp;#39;,
             &amp;#39;Amelie&amp;#39;
           )       
  )


ggplot(
    data = movies_gather %&amp;gt;% 
        filter(
            mpaa == &amp;#39;R &amp;amp; NC-17&amp;#39; &amp;amp; 
            metric != &amp;#39;avg&amp;#39;), 
    aes(x=as.factor(score), y = metric, col = mpaa)) +
    geom_jitter(alpha = 0.3, size = 3) +
    scale_colour_manual(values = c(&amp;#39;#d7191c&amp;#39;)) +
    geom_point(data = labels_r, size = 4,  col = &amp;#39;black&amp;#39;) +
    geom_label_repel(
        data = labels_r,
        aes(label = name),
        size = 3,
        col = &amp;#39;black&amp;#39;) +
    guides(fill = FALSE) +
    xlab(&amp;#39;&amp;#39;) + ylab(&amp;#39;&amp;#39;) +
  theme +
  labs(
    title = &amp;#39;R &amp;amp; NC-17 Movies aren\&amp;#39;t always violent or vulgar... \nbut they sure are profane&amp;#39;,
    subtitle = &amp;#39;Most R and NC-17 movies are 5 or more on a profanity scale&amp;#39;,
    caption = caption)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/profanity2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Indeed, movies in R &amp;amp; NC-17 categories are widely distributed across violence and sex, but snuggle tightly in the upper section of profanity. Why is that? Looking at the data, we can tell that often profanity accompanies other “R” worthy content. However, it is not always the case, and correlation is relatively weak. &lt;a id=&#34;quote&#34;&gt;Good Will Hunting is neither violent nor sexually explicit, but it is profane AF, and, sure enough, is R rated for - wait for it - &lt;em&gt;“strong language, including some sex-related dialogue”.&lt;/em&gt; &lt;/a&gt; It could be just me (after all, I am a foreigner, and English words don’t carry the same connotation for me), but I think it is mighty unfair to Good Will Hunting to be rated R, especially knowing that Scary Movie, parts 3 through 5, are rated PG-13.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(
    data = movies_gather %&amp;gt;% 
        filter(metric != &amp;#39;avg&amp;#39;), 
    aes(x=as.factor(score), y = metric, col = mpaa)) +
    geom_jitter(alpha = 0.2, size = 2) +
    scale_colour_brewer(palette = &amp;quot;Spectral&amp;quot;, direction = -1) +
    geom_point(data = movies_gather %&amp;gt;% 
        filter(name == &amp;#39;Good Will Hunting&amp;#39; &amp;amp; metric != &amp;#39;avg&amp;#39;), 
        size = 7,  
        col = &amp;#39;grey20&amp;#39;,
        alpha = 0.9) +
     geom_point(data = movies_gather %&amp;gt;% 
        filter(name == &amp;#39;Good Will Hunting&amp;#39; &amp;amp; metric != &amp;#39;avg&amp;#39;), 
        size = 2,  
        col = &amp;#39;grey10&amp;#39;) +
    guides(fill = FALSE) +
    xlab(&amp;#39;&amp;#39;) + ylab(&amp;#39;&amp;#39;) +
  theme +
  labs(
    title = &amp;#39;Good Will Hunting, rated R&amp;#39;,
    subtitle = &amp;#39;For strong language, including some sex-related dialogue&amp;#39;,
    caption = caption)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/good_will_hunting-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;So, what have we learned?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is probably OK to use MPAA ratings as a guide&lt;/li&gt;
&lt;li&gt;If you’re optimizing for lack of violence, G and PG movies aren’t that much different, therefore don’t worry much.&lt;/li&gt;
&lt;li&gt;R rating doesn’t mean the movie is violent or has a lot of sexual content. But it definitely means there is some profanity in it!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Caveats&lt;/h1&gt;
&lt;p&gt;It is important to remember that any rating will be arbitrary a priori. We aren’t working with exact count of swear words, time duration of violent scenes, or percentage of naked body revealed. And &lt;a href=&#34;http://kids-in-mind.com&#34;&gt;kids-in-mind.com&lt;/a&gt; rating isn’t perfect either. For example, the website rates Pulp Fiction at 10 on a “sex &amp;amp; nudity” scale, while there is hardly any sexual content in the movie.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Outro&lt;/h1&gt;
&lt;p&gt;This post was nothing but an exercise in &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;blogdown&lt;/code&gt;, &lt;code&gt;ggplot&lt;/code&gt; and &lt;code&gt;rvest&lt;/code&gt; packages, along with honing my data storytelling and writing skills though. I am sure if I torture this dataset a bit more, it may confess to many other things. So, I will wrap up for now, but plan to return to this topic and this dataset in the future. Let me know if there are any specific questions that come to your mind for this dataset!&lt;/p&gt;
&lt;div id=&#34;r-script&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R script&lt;/h3&gt;
&lt;p&gt;This site runs on &lt;del&gt;dunkin&lt;/del&gt; &lt;code&gt;blogdown&lt;/code&gt;, and therefore all the source files are available on GitHub. Now, I must be honest: I am new to Github, to Hugo, and to the web in general. I have very little idea about what I’m doing. But I guess here is the link? &lt;a href=&#34;https://github.com/taraskaduk/taraskaduk/tree/master/content/post/movies&#34; class=&#34;uri&#34;&gt;https://github.com/taraskaduk/taraskaduk/tree/master/content/post/movies&lt;/a&gt; (unless I change something or screw it up big time, in which case throw rocks at me in the comment section)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
