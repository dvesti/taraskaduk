<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Taras Kaduk on Taras Kaduk</title>
    <link>/</link>
    <description>Recent content in Taras Kaduk on Taras Kaduk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Power Query: Excel&#39;s gateway to reproducible analysis</title>
      <link>/2018/03/29/power-query/</link>
      <pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/29/power-query/</guid>
      <description>&lt;div id=&#34;intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;In this blog post, I’ll try to highlight some of Excel’s functionality which have been around for a while, but remains largely unknown to the broad public.&lt;/p&gt;
&lt;p&gt;Now, I’ll be the first one to throw rocks at the Excel camp. I’ve got receipts:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Please don&amp;#39;t... &lt;a href=&#34;https://t.co/r3j3KQtcCT&#34;&gt;pic.twitter.com/r3j3KQtcCT&lt;/a&gt;&lt;/p&gt;&amp;mdash; Taras Kaduk (@taraskaduk) &lt;a href=&#34;https://twitter.com/taraskaduk/status/962376095323643910?ref_src=twsrc%5Etfw&#34;&gt;February 10, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My analysis is in Excel. &lt;a href=&#34;https://twitter.com/hashtag/Loseyourjobin5words?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Loseyourjobin5words&lt;/a&gt;&lt;/p&gt;&amp;mdash; Taras Kaduk (@taraskaduk) &lt;a href=&#34;https://twitter.com/taraskaduk/status/970708905251737605?ref_src=twsrc%5Etfw&#34;&gt;March 5, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Reproducible analysis, case in point. A colleague from another department is out. People come to me w/ requests to recreate some analysis that person did. I have no idea, but OK, let&amp;#39;s see. &lt;br&gt;Colleague&amp;#39;s analysis is in Excel. How can I reproduce it? &lt;br&gt;No analysis for you! &lt;a href=&#34;https://t.co/SoXZO10ekG&#34;&gt;pic.twitter.com/SoXZO10ekG&lt;/a&gt;&lt;/p&gt;&amp;mdash; Taras Kaduk (@taraskaduk) &lt;a href=&#34;https://twitter.com/taraskaduk/status/935263135413555203?ref_src=twsrc%5Etfw&#34;&gt;November 27, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;However, I’ll also be the first to jump in Excel’s defense whenever an opportunity presents itself: &lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Generally a very good article on advantages of R, but as always - completely overlooking Excel enhancements that came with Power Query and Power Pivot (e.g. scripting, hundreds of data connectors. I.e. all the Power BI functionality)&lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/PowerBI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#PowerBI&lt;/a&gt; &lt;a href=&#34;https://t.co/k3qj8px81b&#34;&gt;https://t.co/k3qj8px81b&lt;/a&gt;&lt;/p&gt;&amp;mdash; Taras Kaduk (@taraskaduk) &lt;a href=&#34;https://twitter.com/taraskaduk/status/946035002571218944?ref_src=twsrc%5Etfw&#34;&gt;December 27, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;What gives? I generally agree that Excel is a bad way to conduct an analysis. Having said that, there are many enhancements to the product (Windows version mainly) that came out over the last 10 years which are completely overlooked by both Excel users and Excel bashers. One of such enhancements, Power Query (or Query Editor, or M language), allows for a reproducible data import and transformation, and is quite easy to learn. That will be this post’s topic.&lt;/p&gt;
&lt;div id=&#34;what-this-post-isnt-about&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What this post isn’t about&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Excel cell formulas&lt;/li&gt;
&lt;li&gt;VBA&lt;/li&gt;
&lt;li&gt;Power Pivot and data modeling&lt;/li&gt;
&lt;li&gt;DAX and Excel / Power BI measures&lt;/li&gt;
&lt;li&gt;Power Query’s and M’s history&lt;/li&gt;
&lt;li&gt;full functionality of Power Query&lt;/li&gt;
&lt;li&gt;Excel on a Mac&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-audience&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The audience&lt;/h3&gt;
&lt;p&gt;This post isn’t a guide to using Excel’s and Power BI’s Power Query. This post is not for people who want to learn new cool tricks of Excel.&lt;/p&gt;
&lt;p&gt;This post is for data scientists and analysts who put reproducible analysis (achieved via programming one’s analysis in R or Python) at the center of what they do. This post is for people who like to use Excel as a punching bag (and again, I am one of those people).&lt;/p&gt;
&lt;p&gt;Lastly, this post is for those analysts stuggling to switch the workflow at their place of work from Excel to R or Python. I think that Power Query could act as a gateway drug to reproducible analysis. Meet them where they’re at (in Excel), and guide them by hand out into the world of “programming your analysis”.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;power-query&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power Query&lt;/h2&gt;
&lt;p&gt;Power Query, a.k.a. Data Explorer, a.k.a. Query Editor is Microsoft’s module within Excel and Power BI which allows users to perform data import and transformations before loading the ready tables into a workbook. It’s been around since 2013. Google for more information.&lt;/p&gt;
&lt;div id=&#34;components&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Components&lt;/h3&gt;
&lt;p&gt;In the simplest terms possible, I could break the Power Query down into two parts: the language and the GUI.&lt;/p&gt;
&lt;div id=&#34;the-m-language&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The M language&lt;/h4&gt;
&lt;p&gt;From &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/mt211003.aspx&#34;&gt;MSDN&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Power Query M formula language is optimized for building highly flexible data mashup queries. It’s a functional, case sensitive language similar to F#&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In simpler terms, it is a data transformation language. Now, I’m not a computer scientist, and won’t be able to explain all the technical details well, therefore I suggest that the most interested ones go and check out the &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/mt807488.aspx&#34;&gt;Power Query M language specification&lt;/a&gt;. For the rest of us, I’ll just say that M works by calling a function on a table or a list, then storing this result as a new table, and then calling this new table in the next step with another function. That is oversimplification, of course, but for the purpose of this post, it should do.&lt;/p&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Example 1&lt;/h5&gt;
&lt;p&gt;Let me explain it on an example. In Excel, I created sample table of 3 rows and 3 columns called &lt;code&gt;df&lt;/code&gt;. &lt;img src=&#34;/post/power-query/test1.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I then loaded it into the query editor, and pressed a few buttons. Here is the code it generated (I edited the step names and indented the lines):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;let
    Source = Excel.CurrentWorkbook(){[Name=&amp;quot;df&amp;quot;]}[Content],
    change_type = Table.TransformColumnTypes(Source,
                                            {
                                              {&amp;quot;a&amp;quot;, Int64.Type}, 
                                              {&amp;quot;b&amp;quot;, Int64.Type}, 
                                              {&amp;quot;c&amp;quot;, Int64.Type}
                                            }),
    filter = Table.SelectRows(change_type, each ([a] &amp;lt;&amp;gt; 7)),
    remove_cols = Table.RemoveColumns(filter,{&amp;quot;c&amp;quot;})
in
    remove_cols&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final output will look like this: &lt;img src=&#34;/post/power-query/test2.png&#34; style=&#34;width:50.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first step, &lt;code&gt;Source&lt;/code&gt;, is our import step. It tells Power Query where to find our table. The second step, &lt;code&gt;change_type&amp;quot;&lt;/code&gt;, is auto-generated. Notice that it references the first step as the first argument of the function &lt;code&gt;Table.TransformColumnTypes&lt;/code&gt;: it says &lt;em&gt;“that’s the table we will work with”&lt;/em&gt;. All this step does is assigns columns a, b and c the type of integer. Next step, &lt;code&gt;filter&lt;/code&gt;, references the previous step, and performs a filter operation. Finally, &lt;code&gt;remove_cols&lt;/code&gt; takes the result of the previous step, and then removes a column. Then, the code tells Power Query that the result of &lt;code&gt;remove_cols&lt;/code&gt; is the one to be printed.&lt;/p&gt;
&lt;p&gt;This is a very basic explanation of how this code works. You can twist it, bend it to your will, skip steps, branch out, use parameters etc. But the common functionally is this stitched &lt;em&gt;freight train&lt;/em&gt;-like sequence of steps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Example 2&lt;/h5&gt;
&lt;p&gt;Here is the data transformation sample from &lt;a href=&#34;https://twitter.com/drob&#34;&gt;David Robinson’s&lt;/a&gt; &lt;a href=&#34;https://campus.datacamp.com/courses/introduction-to-the-tidyverse/data-wrangling-1?ex=12&#34;&gt;DataCamp course on the tidyverse&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gapminder)
library(dplyr)

gapminder %&amp;gt;%
  filter(year == &amp;#39;2007&amp;#39;) %&amp;gt;%
  mutate(lifeExpMonths = 12 * lifeExp) %&amp;gt;%
  arrange(desc(lifeExpMonths))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope I don’t need to read to you what it does (if you can’t read the code, try running it. If you have no idea what’s going on here - I suggest taking the above-mentioned David’s class on the tidyverse)&lt;/p&gt;
&lt;p&gt;Here is how I’d solve the same simple task in Power Query. First, Power Query in Excel, unlike Power Query in Power BI, can’t run R scripts, therefore I can’t just load a package. But Power Query can read &lt;code&gt;.RData&lt;/code&gt; files. It can also load stuff from the web. We’ll do just that&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;let
    Source = Web.Page(
                Web.Contents(&amp;quot;https://github.com/jennybc/gapminder/blob/master/inst/extdata/gapminder.tsv&amp;quot;)),
    Data = Source{0}[Data],
    col_types = Table.TransformColumnTypes(Data,{
                                                  {&amp;quot;&amp;quot;, type text}, 
                                                  {&amp;quot;country&amp;quot;, type text}, 
                                                  {&amp;quot;continent&amp;quot;, type text}, 
                                                  {&amp;quot;year&amp;quot;, Int64.Type}, 
                                                  {&amp;quot;lifeExp&amp;quot;, type number}, 
                                                  {&amp;quot;pop&amp;quot;, Int64.Type}, 
                                                  {&amp;quot;gdpPercap&amp;quot;, type number}
                                                }),
    filter = Table.SelectRows(col_types, each ([year] = 2007)),
    mutate = Table.AddColumn(filter, &amp;quot;lifeExpMonths&amp;quot;, each [lifeExp] * 12, type number),
    arrange = Table.Sort(mutate,{{&amp;quot;lifeExpMonths&amp;quot;, Order.Descending}})
in
    arrange&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, again, I changed the names of the steps and indented the code for readability purposes. The rest was generated by Power Query and I was just clicking on things. I want to stress it out again: &lt;strong&gt;I didn’t have to know any of the functions, any of the syntax&lt;/strong&gt;. All I did was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pass a web link into GUI.&lt;/li&gt;
&lt;li&gt;From there, Power Query figured out that it needed a combo of &lt;code&gt;Web.Page(Web.Contents())&lt;/code&gt; to get to the data. It saw a table and guessed column types for me.&lt;/li&gt;
&lt;li&gt;From here, I clicked on the “year” column header to filter it, clicked a button to create a new column out of the old one, and the clicked on its header to sort in descending order.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, &lt;em&gt;I came to this with no pre-existing knowledge of coding, and got myself a reproducible piece of code&lt;/em&gt;. The data refreshes upon each load: Excel will be checking Jenny’s GitHub page every time we refresh the data, and will be applying the steps as documented.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-gui&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The GUI&lt;/h3&gt;
&lt;p&gt;As you may have guessed from my previous paragraph, the Query Editor GUI is bread and butter of this whole scheme: the M language itself is hard to type by hand, the functions are long, it is case sensitive, and there is no good source code editor (Notepad++ and other text editors do a better job than the Power Query itself). But I feel like Power Query wasn’t built to program in: that’s not the main customer base of Excel and Power BI. What Power Query is good at is its GUI that allows users to click around and apply data transformation steps, all the while generating a script behind the scenes.&lt;/p&gt;
&lt;p&gt;First, there is a ribbon with several tabs and plenty of buttons to click on. Some represent very simple existing functions, while others are pretty complicated and generate a solid chunk of code on just one click.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/power-query/1.png&#34; /&gt; &lt;img src=&#34;/post/power-query/2.png&#34; /&gt; &lt;img src=&#34;/post/power-query/3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, you are allowed to interact with your data to some extend. You can’t edit any cells, but you can filter columns, move them around, fill them down, sort, and so on, within the table itself. Power Query will pick up on your actions and will save your transformations in a script.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/power-query/gui1.png&#34; /&gt; &lt;img src=&#34;/post/power-query/gui2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can have more than one query, coming from different sources, and you can make them interact with each other: merge (join), append (union), reference, split, nest and so on.&lt;/p&gt;
&lt;p&gt;You can also re-arrange the query steps in the GUI via simple drag-n-drop, and your script will be re-written to reflect the new order.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-i-like-about-power-query&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What I like about Power Query&lt;/h2&gt;
&lt;p&gt;There are quite a few things I like about Power Query:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning curve.&lt;/strong&gt; It’s easy to get started with Power Query and create functional reproducible scripts out of the gate. The powerful GUI allows for that. No setup necessary: no installation, fine tuning, no ODBC drivers and connections. It just works out of the box. Almost like it wasn’t Microsoft creating it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accessing data.&lt;/strong&gt; Getting the data is the most seamless experience I’ve had. It has pre-built functions for many data sources (I’ve got receipts: &lt;a href=&#34;https://msdn.microsoft.com/en-US/library/mt296615.aspx&#34;&gt;here is the list of functions&lt;/a&gt;), and can recognize a large amount of data formats. It doesn’t require any ODBC setup. The Power Query flavor that runs on Power BI can also use &lt;strong&gt;R scripts&lt;/strong&gt; as a data source or a data step, yet this functionality isn’t a part of Excel yet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tidy-like data storage and display.&lt;/strong&gt; Surprised? Yeah, with a few exceptions, Power Query treats everything as a data frame. What if it’s not a data frame? Then it tries to fit the data into a rectangular shape. I’d like to show a few examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an example of how Power Query treats JSON files. I used the well-known (thanks to Jenny Bryan’s tutorials!) &lt;a href=&#34;https://anapioficeandfire.com/&#34;&gt;API of Ice and Fire&lt;/a&gt;. Here is what I’ve got after a few clicks. Note that I only plugged in the API call as a URL - Power Query did the rest.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/post/power-query/got-1.png&#34;&gt;&lt;img src=&#34;/post/power-query/got-1.png&#34; /&gt;&lt;/a&gt; It is a data frame, but it has a nested list column for titles. Neat!&lt;/p&gt;
&lt;p&gt;Not only JSON records get nested. You could have a nested table (can happen upon a join or after a &lt;code&gt;group_by&lt;/code&gt;-like call), or a nested list. Regardless, Power Query will always try to make your data rectangular, which is pleasing to any &lt;code&gt;tidyverse&lt;/code&gt; adept. Here is another example. I took the same Gapminder dataset, and nested it, grouping by &lt;code&gt;country&lt;/code&gt;. The table above is now how Power Query sees the table. The data frame below is a sneak peek into one of the nested cells for the United Kingdom. The function up on top is the step I applied to nest the data frame: &lt;img src=&#34;/post/power-query/gui4.png&#34; /&gt; One more example. Here is how Power Query sees a folder full of files: &lt;img src=&#34;/post/power-query/gui5.png&#34; /&gt; Likewise, if you told Power Query to access a database, and didn’t specify SQL statement, it would return a data frame of all tables and views in that database, and you can take it from there.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Robust GUI that keeps the code.&lt;/em&gt; Another cool feature is that the GUI can handle 90% of one’s needs, and it scripts all transformations behind the scenes. I think of it as a gateway drug to &lt;em&gt;“programming one’s analysis”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-i-dont-like-about-power-query&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What I don’t like about Power Query&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speed.&lt;/strong&gt; It is unbelievably slow. I guess the convenience comes at a cost of performance. It does well on small datasets and simpler operations, but fails to do a decent job the moment you scale.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Syntax and flexibility.&lt;/strong&gt; The language is rather rigid, and the syntax is annoying at times.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limits.&lt;/strong&gt; It can only do so much. You can import and transform the data, but you can’t do anything else here.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;What we have these days is several generations of knowledge workers trained on doing their analysis in Excel. Just bashing the tool is not productive. We think we provide better alternatives with R or Python, but we frequently forget about the learning curve associated not only with learning a new language, but also with learning &lt;strong&gt;a language&lt;/strong&gt; for the first time. Excel’s Power Query could serve as an important stepping stone in taking the analysis out of the Wild West world of Excel VLOOKUPs into the world of reproducible code, git repos and other warm and fuzzy things.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R and Power BI - Better Together</title>
      <link>/talk/jpug-r/</link>
      <pubDate>Thu, 08 Mar 2018 18:00:00 -0500</pubDate>
      
      <guid>/talk/jpug-r/</guid>
      <description>&lt;p&gt;R is one of the leading languages in the world of data science. It has a growing following and many applications, including data transformation, machine learning, statistical modeling, data visualization, web applications, and even building websites.&lt;/p&gt;

&lt;p&gt;R is widely used in academia, science and business. Some of the large companies like Microsoft, Facebook, Airbnb, Etsy, Stitch Fix, IBM and Uber use R for data science exclusively, or in tandem with Python.&lt;/p&gt;

&lt;p&gt;In this session, we will first look at R separately as a language, followed by use of R within Power BI: how, when and why to resort to it for data transformation, modeling and visualization.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Imposterism in Data Science: Addressing the credentials problem</title>
      <link>/2018/01/22/impostor/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/22/impostor/</guid>
      <description>&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/impostor/fraud.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/imposterism-in-data-science-f96c29ae96ce&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt; &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We happen to talk a lot about the impostor syndrome these days. No wonder — it seems to be an important subject. But what is it? That feeling of faking it while others &lt;em&gt;clearly&lt;/em&gt; know what’s they’re doing.&lt;/p&gt;
&lt;p&gt;Many attempts have been made to clarify the issue. Explaining that it is OK, that we all feel that way going through life. Et cetera et cetera. Some advice has gone as far as making the impostor syndrome a badge of honor. But that’s on the extreme side, and I’ll save this topic for another time…&lt;/p&gt;
&lt;p&gt;Today, I want to pick apart one particular advice on how to combat the feeling of being a fraud. That is, to roll up your sleeves and start learning. I’ve heard it many times before, and always wanted to respond, but never had the time. Because I was, you know, learning.&lt;/p&gt;
&lt;div id=&#34;curb-your-imposterism-start-meta-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Curb your imposterism, start meta-learning?&lt;/h2&gt;
&lt;p&gt;The blog post that triggered this post was written a few days ago by a fellow data scientist &lt;a href=&#34;https://twitter.com/edwin_thoen&#34;&gt;Edwin Thoen&lt;/a&gt; and is titled &lt;a href=&#34;https://edwinth.github.io/meta-learning/&#34;&gt;&lt;em&gt;“Curb your imposterism, start meta-learning”&lt;/em&gt;&lt;/a&gt;. In it, Edwin highlights the importance of choosing what to focus on and what to learn, and then learning it.&lt;/p&gt;
&lt;p&gt;First off, I think this is a great advice in general. In fact, I typically follow somewhat similar guidelines in learning new things. But this in no way helps me combat my impostor syndrome.&lt;/p&gt;
&lt;p&gt;Secondly, I must point out that the author does acknowledge his advice is not universal. Edwin says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Now, what works for me might not work for you. Maybe a different system fits you better. However, I think everybody benefits from defining the data scientist he/she is and actively choose what not to learn. So, what I’m going to say next is not a disagreement with this approach, but rather an important addition that I feel is constantly overlooked.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;do-you-even-phd-bro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Do you even PhD, bro?&lt;/h2&gt;
&lt;p&gt;There are jobs out there where your degree means everything. Lawyers and doctors are a good example of this category. And there are occupations where what matters the most is what you &lt;strong&gt;can&lt;/strong&gt; do, while your degree (or lack of thereof) is a secondary issue at most.&lt;/p&gt;
&lt;p&gt;I find Data Science increasingly leaning towards the former category. Scan the posted jobs for data science related positions: you’ll see a Master’s degree desired and a Bachelor’s degree being a mandatory requirement. Higher position? PhDs please.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;faking-it-in-the-world-of-educated-people&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Faking it in the world of educated people&lt;/h2&gt;
&lt;p&gt;If you think that this is a sound request, you won’t be wrong. But let me speak about it from my own experience. I’ve been in the workforce for 10 years, have been doing analytics for over 5 years. And I don’t have a degree. No, I don’t mean MS or PhD. I don’t have a Bachelor’s degree.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/post/impostor/yachty.gif&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;Yep. And it’s hard to admit. Ever since I had to leave one of the best Economics universities of my country for a full-time job to make ends meet, I felt ashamed of the fact that I never graduated. And the feeling became even stronger as I broke into the analytics / data science field. Whereas overall a Bachelor’s degree in combination with hard work and continuous learning is fairly enough to hold a well-paid job, in our field it is an absolute bare minimum, but you need much more.&lt;/p&gt;
&lt;p&gt;So how do you think it feels when you come across a position you have all the skills for, but which requires an MS in Math, Computer Science or Statistics? You’re like &lt;em&gt;“I’m 2 steps below that!”&lt;/em&gt; Or how does it feel when your direct report whom you teach, mentor and guide has a Master’s degree?&lt;/p&gt;
&lt;p&gt;It doesn’t feel terrible. In fact, it feels just fine. But your impostor syndrome feeds off that.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;no-silver-bullet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;No silver bullet&lt;/h2&gt;
&lt;p&gt;There aren’t many of us. Somehow, data science is different from even computer science. Credentials matter here. And most of the people do have at least a bare minimum of a Bachelor’s degree.&lt;/p&gt;
&lt;div id=&#34;but-for-the-small-amount-of-folks-like-me-who-work-or-want-to-work-in-analytics-data-science-yet-dont-have-any-degree-beyond-high-school-here-is-some-bitter-truth-there-is-no-silver-bullet.-go-and-study-formally.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;span style=&#34;background-color: #DDDDDD&#34;&gt; But for the small amount of folks like me, who work (or want to work) in analytics / data science, yet don’t have any degree beyond high school, here is some bitter truth: there is no silver bullet. Go and study formally.&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;All these ten years, I though I can prove the world wrong, thought I can show that one can be smart without a diploma from a 4-year university. MOOCs, online classes, free and paid resources, books, guides… It all takes you only so far.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/post/impostor/homer.gif&#34; style=&#34;width:75.0%&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;And I’m tired of fighting this uphill battle. That’s why I’m starting again from ground zero. Getting an AA at a state college, followed by a BS and hopefully an MS at a state university. And we’ll see what’s next. Classes start next week. It’s going to be long and costly, but it’s going to be worth it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The 4 Stages of Data Analytics Maturity: Challenging the Gartner’s Model</title>
      <link>/2018/01/22/4-stages/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/22/4-stages/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://www.linkedin.com/pulse/4-stages-data-analytics-maturity-challenging-gartners-taras-kaduk/&#34;&gt;&lt;strong&gt;LinkedIn&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://medium.com/taras-kaduk/4-stages-of-data-analytics-maturity-challenging-gartners-model-590eb5ebe6d1&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt; &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you happen to work in analytics, data science or business intelligence, you’ve probably seen one of the iterations of this Gartner’s graph on stages of data analysis in a company:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/4-stages/gartner.jpg&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The figure above shows various stages of analytics maturity, from “descriptive” to “prescriptive”. I’ve seen it so many times, it became an eyesore to me.&lt;/p&gt;
&lt;p&gt;There is nothing wrong with it. This look nicely breaks down the evolution of analytics into understandable parts and pairs each stage with a question to be answered: &lt;em&gt;what happened, why did it happen, what will happen, how can we make it happen&lt;/em&gt;. And exactly this cadence of words &lt;em&gt;what, why, what, how&lt;/em&gt; is what made me think that the relation between the 4 stages is not exactly linear.&lt;/p&gt;
&lt;p&gt;In my mind, the &lt;em&gt;what&lt;/em&gt; questions (descriptive and predictive analytics) can simply be answered by what’s in the data: either existing historical data (descriptive analytics) or historical data, extrapolated into the future using machine learning techniques and forecasting (predictive analytics). You can easily move from one stage to another. There is no “diagnostic analytics” step in between.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Why&lt;/em&gt; and &lt;em&gt;how&lt;/em&gt; (diagnostic and prescriptive analytics), on the other hand, are the questions that can be answered with existing data and a dash of business intelligence, either manual (a person going over the numbers and figuring things out), or baked in (an algorithm analyzing the numbers and producing verdicts based on models ran). In other words, both &lt;strong&gt;diagnostic&lt;/strong&gt; and &lt;strong&gt;prescriptive&lt;/strong&gt; analytics build on top of &lt;strong&gt;descriptive&lt;/strong&gt; and &lt;strong&gt;predictive&lt;/strong&gt; analytics respectively.&lt;/p&gt;
&lt;p&gt;So, another way to visualize the connection between the four times would look something like this:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/4-stages/2by2.png&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;One issue with the following graph is that it doesn’t fully show all the ways that &lt;em&gt;data + insight + machine learning&lt;/em&gt; produce 4 flavors of analytics. And hence the good ol’ venn diagram:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/4-stages/venn.png&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Every company’s approach to analytics and data science is still unique: there are very few best practices known in the industry, and we all are still figuring it out. Similarly, every analyst’s view on data analytics evolution and maturity will be different, and many of my colleagues will disagree with this view. And that is fine. We are still in the early stages of learning how to cook the proverbial spaghetti, and therefore let’s not rob ourselves of the joy of throwing stuff from the pot onto the wall and seeing what sticks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On Importance of Minimalism in Retrospective Analytics</title>
      <link>/2018/01/12/sass/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/12/sass/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/on-importance-of-minimalism-in-retrospective-analytics-75c5a02c2c83&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/pulse/importance-minimalism-retrospective-analytics-taras-kaduk-1/&#34;&gt;&lt;strong&gt;LinkedIn&lt;/strong&gt;&lt;/a&gt; &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hey, analyst, how is life? Talk to me. Do you love what you do for life, do you like all things data?&lt;/p&gt;
&lt;p&gt;Yet, do you sometimes feel like you’re a Sisyphus rolling a giant rock of data up the hill every day, only to see it go down with a racket in the evening (and you know what you’re going to do tomorrow)? Or, do you imagine yourself being a plate spinner at a circus, only instead of plates and poles you’ve got five dozen reports to spin, and instead of an entertained crowd you’ve got your co-workers, managers and senior executives watching your “performance” and asking to add more plates, and God forbid any one plate falls?&lt;/p&gt;
&lt;p&gt;The truth is — herding your reporting under one roof is no small feat. It is especially challenging at times, when you need to create a unified business solution from bits and pieces: Excel spreadsheets, Access databases, and built-in reports older than some of your employees. Create it and then maintain it. Oh my.&lt;/p&gt;
&lt;p&gt;It doesn’t help that such enormous task usually receives minimal funding, as you are basically “fixing” something that “ain’t broke”.&lt;/p&gt;
&lt;p&gt;It’s OK. I’ve been there. Coming to the office all excited to work with data and create some mad data viz and publish it, only to get a hard kick in a sensitive area with one report not refreshing, another report having errors, and with a dozen more requests to spin off more of the same.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;the-sass-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The SASS Framework&lt;/h1&gt;
&lt;p&gt;But it doesn’t have to be this way. Over the years, my team and I have managed to develop a simple framework for dealing with this kind of plate-spinning environment. We are a very lean team, but we manage to accomplish a lot, with minimum time spent fixing what’s broken and/or doing anything that resembles what’s been done before.&lt;/p&gt;
&lt;p&gt;The secret sauce here is optimizing for time. You don’t have the capacity to create isolated one-time-only solutions, to fix things on a regular basis, to perform manual updates, to enter data, you’ve got time for none of that. Much like professional cyclists reduce the weight of a bicycle by replacing parts, trimming of edges of nuts and bolts and drilling holes through bike pipes, you need to save as much time as possible by shaving off all the nonessential movements.&lt;/p&gt;
&lt;p&gt;I call our result approach the SASS framework. SASS here stands for simple, automated, standard and scalable. Building your work around these four “standards” would help you get out of the self-perpetuated loop of constant plate-spinning.&lt;/p&gt;
&lt;p&gt;Applying this framework is simple. You ask these 4 questions on simplicity, automation, standard and scale. Furthermore, they are applicable on every stage of you data’s journey: whether you only take on a new request, build out a report, or deploy it for continuous consumption.&lt;/p&gt;
&lt;p&gt;Below, I’ll show you some examples of what it means.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;simple&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/sass/simple.png&#34; alt=&#34;Simple is good. It’s that simple (pun intended).&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Simple is good. It’s that simple (pun intended).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Do you reports suffer from abundance of KPIs, complicated data models, transformation steps no one understands? Take a step back and simplify it. Keep your data tidy and your calculations simple. Your code should be laconic, elegant and easy to read. For me, complicated code is the first sign of a possible trouble. Do you have to many visuals? More opportunities of something failing. Reduce. Do have multiple reports running doing a similar thing? Reduce the amount and consolidate them. Keep it simple.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automated&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automated&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/sass/automated.png&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;“That which does not kill us, makes us stronger.” &lt;strong&gt;~Nietzsche&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well… then redundant work definitely doesn’t make me stronger. It slowly kills me. And it kills me seeing other people doing it.&lt;/p&gt;
&lt;p&gt;A wise man once said: “Life is too short to do what can be automated”. He also said: “Automate until it hurts”. (Sadly, he wasn’t heard and his wisdom almost vanished. But then he decided to write this blog post)&lt;/p&gt;
&lt;p&gt;Do your data sources refresh automatically? Do you script all of your transformations (be it SQL, R, Python, Power Query, or a screen grab script), or do you perform any actions manually (save a file, format a spreadsheet, paste a table etc)? Is your solution on a scheduled refresh, or do you have click on things?&lt;/p&gt;
&lt;p&gt;It is tempting to let some of the manual action slip in as “no big deal”. That’s fine and dandy when you have 5–10 reports to maintain. Scale it up to 50 — and your “no big deal” adds up to staying late and spending weekends running shit you’re not supposed to.&lt;/p&gt;
&lt;p&gt;You’re too valuable to do some script’s dirty job.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standard&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/sass/standard.gif&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This one must be simple and obvious. Keeping things standard again means more time savings: less validation, less discovery time, less training and explaining.&lt;/p&gt;
&lt;p&gt;Do you use one record of truth, the data from one lake? If half your analysts use one source (say, a new database on the cloud) and another half is running stored procedures written before the times of Noah’s Ark, then don’t get surprised if the two teams end up with a different number.&lt;/p&gt;
&lt;p&gt;Do you and/or your team adhere to the same visual design principles? Nothing beats bars and lines, and nothing beats simple colors. Most of our recurring reporting looks pretty plain and boring: it is bars, lines and tables; and the color is designated to carry a specific data point and almost never used just for the looks. Yeah, it isn’t fun, but you avoid the decision fatigue, you save time by not coming up with a new palette and new outline, and you don’t spend time explaining your audience what this new spider web chart means and how to read it. Don’t get me wrong, I’m not against new reports, cool colors, interesting and fresh charts — I’m just showing you where to save time and resources when you need to maintain a large number of such reports.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scalable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scalable&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/sass/scalable.gif&#34; style=&#34;width:100.0%&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;After all said and and done, you should be able to repurpose your logic for something else. Or scale it within the given context. Similarly, many solutions can be created on top of some previous work, instead of creating a new solution. This also makes sure your solutions are “standardized”: you kill 2 birds with 1 stone.&lt;/p&gt;
&lt;p&gt;In our line of work, we’ve created several large data models for financial, sales and operational data, and most of the existing and new reports feed off these data models. We save a lot of time deploying new solutions because most of the work is already done and the data models have been built in such a way that we easily can scale it for something new.&lt;/p&gt;
&lt;p&gt;Of course, this minimalist approach in retrospective analytics is not for everyone. There are different roles in the world of data science, and different tactics work for each role. The SASS framework is mostly applicable to the teams or individuals responsible for running a large suite of recurring reporting within a mid to large size company.&lt;/p&gt;
&lt;p&gt;Let me know if you’re facing similar struggles at work and if you have different solutions in place!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>When your data analysis is validated on The Late Show with Stephen Colbert</title>
      <link>/2018/01/10/mpaa-damon/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/10/mpaa-damon/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/when-your-data-analysis-is-validated-on-the-late-show-with-stephen-colbert-b7cd6ca37147&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;About a month ago, I wrote a little article about the MPAA rating system. I set up to find out if their lettering system does any justice to the actual content seen on the screen. Briefly speaking, it does, but with caveats.&lt;/p&gt;
&lt;p&gt;One of such caveats was the effect of profanity. What my quick and dirty data analysis showed was that profanity was the sure thing that could send a movie into an R category:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/profanity-1.png&#34; style=&#34;width:100.0%&#34; /&gt; &lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/profanity2-1.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I even wrote this in my &lt;a href=&#34;/post/mpaa/mpaa/#quote&#34;&gt;blog post&lt;/a&gt;, when summing up the effects of profanity on movie’s rating:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Good Will Hunting is neither violent nor sexually explicit, but it is profane AF, and, sure enough, is R rated for - wait for it - &lt;em&gt;“strong language, including some sex-related dialogue”.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;when-matt-damon-proves-you-right&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;When Matt Damon proves you right&lt;/h2&gt;
&lt;p&gt;So, a few days ago we were watching &lt;strong&gt;The Late Show with Stephen Colbert&lt;/strong&gt;, and this bit with Matt Damon caught my instant attention:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I remember when people at MIRAMAX came to us and said &lt;em&gt;“Could you make it [Good Will Hunting] &lt;strong&gt;PG-13?&lt;/strong&gt;”&lt;/em&gt; There’s no violence or sex to speak of, it’s just…&lt;/p&gt;
&lt;p&gt;And I said &lt;em&gt;“What’s making it rated &lt;strong&gt;R&lt;/strong&gt;?”&lt;/em&gt;, and they said &lt;em&gt;“the language”&lt;/em&gt;, and I said &lt;em&gt;“Okay well so we could loop a couple lines”&lt;/em&gt;, and they go &lt;em&gt;“Yeah but you’re only allowed”&lt;/em&gt;… I think at the time you were allowed to say the &lt;strong&gt;F-word&lt;/strong&gt; three times, and I said &lt;em&gt;“Okay, well how many are we off by?”&lt;/em&gt; And they said &lt;em&gt;“You go over by a hundred and forty-five”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/2GrKY7Qqal8?start=220&amp;amp;end=305&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;Ha! So, my theory checks out! It’s profanity that makes a movie R rated! It can be puritan and pacifistic, but you drop a couple of F-bombs — and you’re out.&lt;/p&gt;
&lt;p&gt;It is funny that I chose exactly Good Will Hunting as an example of how an otherwise modest movie can be sent straight to the R bench for what Matt claims is how they all talk in Boston.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/good_will_hunting-1.png&#34; /&gt;

&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;Well, anyway, that’s it folks. Do more data analysis, for work and for fun.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Create World Pixel Maps in R</title>
      <link>/2017/11/26/pixel-maps/</link>
      <pubDate>Sun, 26 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/26/pixel-maps/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/r-walkthrough-create-a-pixel-map-537ce12c2f0c&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Today, I’m going to show you how to make pixel maps in R. Why pixel maps? Because they look awesome!&lt;/p&gt;
&lt;p&gt;I was searching on the web for a while, but couldn’t find a good tutorial. Being stubborn as I am, I eventually figured out a way to get what I want. You know, if you torture your code enough, it might give you what you need.&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;First, of course, loading required packages. These days, I don’t bother with discrete packages and get the entire &lt;code&gt;tidyverse&lt;/code&gt; right away. Aside from that, you may need the &lt;code&gt;ggmap&lt;/code&gt; package, which I used in the earlier iterations of this script (more on that later). You’ll also need the &lt;code&gt;maps&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Library -----------------------------------------------------------------

library(tidyverse)
library(googlesheets)
library(maps)
library(here)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll need our data points. You can do anything you want here: load a google sheet with data, reach to you Google Maps data, import a csv file, whatever your heart desires.&lt;/p&gt;
&lt;p&gt;Initially, I created a data frame with places I’ve been to, and then grabbed their coordinates with &lt;code&gt;mutate_geocode()&lt;/code&gt; function from a &lt;code&gt;ggmap&lt;/code&gt; package. That piece of code takes a while to run, and the list doesn’t really change that much, so I ended up saving it as a separate Google sheet, and now I simply import it. But you do as you wish.&lt;/p&gt;
&lt;p&gt;You’ll obviously need to replace this chunk with your own data. I include &lt;code&gt;tail&lt;/code&gt; of my tibble to give you an idea about the data structure&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get data -------------------------------------------------------------
url &amp;lt;- &amp;#39;https://docs.google.com/spreadsheets/d/e/2PACX-1vQoRxmeOvIAQSqOtr2DMOBW_P4idYKzRmVtT7lpqwoH7ZWAonRwOcKR2GqE-yqUOhb5Ac_RUs4MBICe/pub?output=csv&amp;#39;
destfile &amp;lt;- &amp;quot;locations.csv&amp;quot;
curl::curl_download(url, destfile)
locations &amp;lt;- read_csv(destfile)
tail(locations)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   city                    status   lon   lat family
##   &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;
## 1 Hurgada, Egypt          been    33.8  27.3      2
## 2 Simferopol, Ukraine     been    34.1  45.0      2
## 3 Yalta, Ukraine          been    34.2  44.5      2
## 4 Dnipropetrivsk, Ukraine been    35.0  48.5      2
## 5 Zaporizhya, Ukraine     been    35.1  47.8      1
## 6 Moscow, Russia          been    37.6  55.8      1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rounding-the-coordinates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rounding the coordinates&lt;/h2&gt;
&lt;p&gt;As I’m creating a pixel map - I need dots in the right places. I’m going to plot a dot for each degree, and therefore I need my coordinates rounded to the nearest degree&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;locations &amp;lt;- locations %&amp;gt;% 
        mutate(long_round = round(lon, 0),
               lat_round = round(lat,0))
tail(locations)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
##   city                    status   lon   lat family long_round lat_round
##   &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Hurgada, Egypt          been    33.8  27.3      2        34.       27.
## 2 Simferopol, Ukraine     been    34.1  45.0      2        34.       45.
## 3 Yalta, Ukraine          been    34.2  44.5      2        34.       44.
## 4 Dnipropetrivsk, Ukraine been    35.0  48.5      2        35.       48.
## 5 Zaporizhya, Ukraine     been    35.1  47.8      1        35.       48.
## 6 Moscow, Russia          been    37.6  55.8      1        38.       56.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;generate-a-pixel-grid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generate a pixel grid&lt;/h2&gt;
&lt;p&gt;The next step is the key to getting a pixel map. We’ll fill the entire plot with a grid of dots - 180 dots from south to north, and 360 dots from east to west, but then only keep the dots that are on land. Simple!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Generate a data frame with all dots -----------------------------------------------

lat &amp;lt;- data_frame(lat = seq(-90, 90, by = 1))
long &amp;lt;- data_frame(long = seq(-180, 180, by = 1))
dots &amp;lt;- lat %&amp;gt;% 
        merge(long, all = TRUE)

## Only include dots that are within borders. Also, exclude lakes.
dots &amp;lt;- dots %&amp;gt;% 
        mutate(country = map.where(&amp;#39;world&amp;#39;, long, lat),
               lakes = map.where(&amp;#39;lakes&amp;#39;, long, lat)) %&amp;gt;% 
        filter(!is.na(country) &amp;amp; is.na(lakes)) %&amp;gt;% 
        select(-lakes)

head(dots)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   lat long                   country
## 1 -83 -173                Antarctica
## 2 -83 -172                Antarctica
## 3 -83 -171                Antarctica
## 4  60 -167 USA:Alaska:Nunivak Island
## 5  60 -166 USA:Alaska:Nunivak Island
## 6  65 -166                USA:Alaska&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;And now the easy part. Plotting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note that this post, just like this entire site, runs on &lt;code&gt;blogdown&lt;/code&gt;, and the post is created via Rmarkdown. When the plots render here - they look ugly-ish due to the fact that geom_point doesn’t scale down along with the plot. The output on your machine will look better. Take a look at the head image to understand how your output may look like&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme &amp;lt;- theme_void() +
        theme(panel.background = element_rect(fill=&amp;quot;#212121&amp;quot;),
              plot.background = element_rect(fill=&amp;quot;#212121&amp;quot;),
              plot.title=element_text(face=&amp;quot;bold&amp;quot;, colour=&amp;quot;#3C3C3C&amp;quot;,size=16),
              plot.subtitle=element_text(colour=&amp;quot;#3C3C3C&amp;quot;,size=12),
              plot.caption = element_text(colour=&amp;quot;#3C3C3C&amp;quot;,size=10),  
              plot.margin = unit(c(0, 0, 0, 0), &amp;quot;cm&amp;quot;))

plot &amp;lt;- ggplot() +   
        #base layer of map dots
        geom_point(data = dots, aes(x=long, y = lat), col = &amp;quot;grey45&amp;quot;, size = 0.7) + 
        #plot all the places I&amp;#39;ve been to
        geom_point(data = locations, aes(x=long_round, y=lat_round), color=&amp;quot;grey80&amp;quot;, size=0.8) + 
        #plot all the places I lived in, using red
        geom_point(data = locations %&amp;gt;% filter(status == &amp;#39;lived&amp;#39;), aes(x=long_round, y=lat_round), color=&amp;quot;red&amp;quot;, size=0.8) +
        #an extra layer of halo around the places I lived in
        geom_point(data = locations %&amp;gt;% filter(status == &amp;#39;lived&amp;#39;), aes(x=long_round, y=lat_round), color=&amp;quot;red&amp;quot;, size=6, alpha = 0.4) +
        #adding my theme
        theme
plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/pixel-maps/pixel-maps_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You probably want to save the map, too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;map_full.jpg&amp;#39;, 
       device = &amp;#39;jpg&amp;#39;, 
       path = getwd(), 
       width = 360, 
       height = 180, 
       units = &amp;#39;mm&amp;#39;,
       dpi = 250)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the map of the entire world can be overwhelming and sad, especially if you, just like me, are not much of a traveler. Look at it! There aren’t many dots! WTF?! Sad!&lt;/p&gt;
&lt;p&gt;You can zoom in on an area you did cover (e.g. include USA only), either computationally (calculate you westernmost, easternmost, southernmost and northernmost points and pass them as xlim and ylim), or excluding continents from the map with &lt;code&gt;dplyr&lt;/code&gt; (excluding Antarctica at least would be a good idea). You can also use a different map to start with - World map may not be necessary for some tasks. I used it because I was fortunate enough to live on 2 continents, but your mileage may vary.&lt;/p&gt;
&lt;p&gt;In all of these cases, you may want to reconsider the grain of the map: if you zoom in on USA only, you may want to choose to plot a dot for every 0.5 degrees, and then would need to adjust your coordinate rounding respectively (round to the nearest half of degree). Why do it? The finer your grain - the more details you’ll get. For instance, with a grain of 1 degree, San Francisco, San Mateo, San Rafael and Oakland are all be one same dot.&lt;/p&gt;
&lt;p&gt;I could definitely program my way though this scaling issue and create a parameter, and make other variables depend on it… I don’t find this exercise to be particularly useful in this case. If you get it done - awesome!&lt;/p&gt;
&lt;p&gt;For my case, I wanted a wide banner, so I chose some specific arbitrary limits that looked good to me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot + scale_y_continuous(limits = c(10, 70), expand = c(0,0)) +
        scale_x_continuous(limits = c(-150,90), expand = c(0,0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/pixel-maps/pixel-maps_files/figure-html/map-wide-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outro&lt;/h2&gt;
&lt;p&gt;Obviously, there is so much more to do with this. The possibilities are endless. The basic idea is pretty simple - generate a point grid and plot rounded coordinates on top of the grid.&lt;/p&gt;
&lt;p&gt;Let me know if you find new implementations of this code!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;repo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Repo&lt;/h2&gt;
&lt;p&gt;As this blog is rendered with &lt;code&gt;blogdown&lt;/code&gt;, all the source code is on Github for your pleasure. taraskaduk.com repo is at &lt;a href=&#34;https://github.com/taraskaduk/taraskaduk/&#34; class=&#34;uri&#34;&gt;https://github.com/taraskaduk/taraskaduk/&lt;/a&gt; and the Rmd file for this post should be located at &lt;a href=&#34;https://github.com/taraskaduk/taraskaduk/tree/master/content/post&#34; class=&#34;uri&#34;&gt;https://github.com/taraskaduk/taraskaduk/tree/master/content/post&lt;/a&gt; (unless I mess it all up and relocate it. I’m really not good at your whole github and blogdown thing. But I’m learning)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SASS: Simplify, Standardize, Automate and Scale your enterprise reporting</title>
      <link>/talk/jpug-sass/</link>
      <pubDate>Thu, 05 Oct 2017 18:00:00 -0400</pubDate>
      
      <guid>/talk/jpug-sass/</guid>
      <description>&lt;p&gt;In this session, we will talk about how to run Power BI reporting on enterprise level with minimal resources (i.e. lean), and introduce you to a SASS framework: simple, standard, automated and scalable.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do MPAA movie ratings mean anything?</title>
      <link>/2017/09/30/mpaa/</link>
      <pubDate>Sat, 30 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/30/mpaa/</guid>
      <description>&lt;p&gt;&lt;em&gt;Cross-posted: &lt;a href=&#34;https://medium.com/taras-kaduk/do-mpaa-movie-ratings-mean-anything-1d1ab683f21d&#34;&gt;&lt;strong&gt;Medium&lt;/strong&gt;&lt;/a&gt; &lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;Being a parent in modern days is lots of fun. Not only all of us are pretty much winging it, not having any idea what we’re doing &lt;em&gt;(seriously, you need a license to do braids and nails, yet raising a human being a future member of society is a no-brainer, right?)&lt;/em&gt;  — we are also constantly being watched and judged by other parents.&lt;/p&gt;
&lt;p&gt;When it comes to watching movies with our six-year-old son, we don’t have a strict set of rules. We pretty much fly by the seat of our pants with &lt;em&gt;“I know it when I see it”&lt;/em&gt; approach to violence, profanity, or any other content. Not to say that we’re watching Pulp Fiction and Basic Instinct (the most challenging movie to date was probably Alice in the Wonderland), but all the movies we watch with our son are between G and PG - and we hardly can tell a difference between the two.&lt;/p&gt;
&lt;p&gt;That’s why I was surprised to find out that some parents swear by this MPAA rating system, and use it religiously when deciding what their kids can and can’t watch.&lt;/p&gt;
&lt;p&gt;And it’d be all good if I haven’t noticed that these ratings are sometimes kind of… &lt;em&gt;arbitrary?&lt;/em&gt; So, I decided to dig into the data. Because data will solve all of our problems, right?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;I searched around a bit, and stumbled upon a few awesome things. First, apparently &lt;a href=&#34;http://www.imdb.com/&#34;&gt;imdb.com&lt;/a&gt; has a parental section for every movie. However, these guides are not standard in the way they are filled out, and scrubbing IMDb for this data wouldn’t get me where I wanted to be fast enough. And then I stumbled upon this awesome website called &lt;a href=&#34;http://kids-in-mind.com&#34;&gt;kids-in-mind.com&lt;/a&gt;. It had a lot of info similar or equal to one contained on &lt;a href=&#34;http://www.imdb.com/&#34;&gt;IMBd.com&lt;/a&gt;, but it had a crucial key component: every movie on this website is rated on an 11-point scale, from 0 to 10, on three metrics: &lt;strong&gt;sex &amp;amp; nudity&lt;/strong&gt;, &lt;strong&gt;violence &amp;amp; gore&lt;/strong&gt;, and &lt;strong&gt;profanity&lt;/strong&gt;. Well, this is just perfect! Not only that — it also has that MPAA rating data point for every movie, which means I get all of my data in one sitting.&lt;/p&gt;
&lt;p&gt;So, I wrote a little R script using &lt;code&gt;rvest&lt;/code&gt; package, and got my data into a tidy data frame, and started exploring. After a little bit of data wrangling (I excluded &lt;strong&gt;NR&lt;/strong&gt; movies as they are obviously &lt;em&gt;not rated&lt;/em&gt;, and are all over the place. Also, Kids In Mind database didn’t have many &lt;strong&gt;NC-17&lt;/strong&gt; rated movies, therefore I combined them with &lt;strong&gt;R&lt;/strong&gt; rated films), I got my first results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library() -------------------------------------------------------------------

library(readr)
library(stringr)
library(tidyverse)
library(rvest)
library(ggrepel)

# Extract -----------------------------------------------------------------
## Here, I decided to be easy on the website and not to scrape it every time: instead, I saved the output and only let the web scraping run is the output isn&amp;#39;t found.

path &amp;lt;- getwd()

if (file.exists(paste(path,&amp;#39;movies_extract.csv&amp;#39;, sep = &amp;quot;/&amp;quot;))) {
  movies_df &amp;lt;- read_csv(paste(path,&amp;#39;movies_extract.csv&amp;#39;, sep = &amp;quot;/&amp;quot;))
} else {
        
        
## This is my first attempt ever to use rvest and also the first time I work with regex. Obviously, the code need improvement! Comments are welcome!

        
## The site has data displayed in a table, broken down into pages, and the url contains the top movie&amp;#39;s position and a range of 3 metrics - sex, violence and profanity - from 0 to 10.
## The way the data comes out after scraping, I had a problem with movies rated 10 on profanity, as didn&amp;#39;t know how to let my regex know that 10 is 10 (it took all 10 as 1, and appended 0 to the next movie in line)
## I couldn&amp;#39;t figure it out!!! So, I made a loop inside the loop. That&amp;#39;s what j and k variables are for.
        
  movies_vector &amp;lt;- character()
  j&amp;lt;-10
  k&amp;lt;-0
  i&amp;lt;-0
 
  for(j in 10:9) {
    if (j == 9) k &amp;lt;- 0 else k &amp;lt;- 10
    position &amp;lt;- 0
    
    for(i in 0:300){
      url &amp;lt;- paste(&amp;quot;http://www.kids-in-mind.com/cgi-bin/listbyrating/search.pl?query=&amp;amp;stpos=&amp;quot;, 
                   position, 
                   &amp;quot;&amp;amp;stype=AND&amp;amp;s1=0&amp;amp;s2=10&amp;amp;v1=0&amp;amp;v2=10&amp;amp;p1=&amp;quot;,
                   k,
                   &amp;quot;&amp;amp;p2=&amp;quot;,
                   j,
                   &amp;quot;&amp;amp;m=1&amp;amp;m=2&amp;amp;m=3&amp;amp;m=4&amp;quot;, sep = &amp;quot;&amp;quot;)
      
      import &amp;lt;- read_html(url)
      
      vector &amp;lt;- import %&amp;gt;%
        html_node(&amp;quot;.t11normal+ p&amp;quot;) %&amp;gt;%
        html_text() %&amp;gt;% 
        str_replace_all(&amp;#39;\\\n\\\n&amp;#39;, &amp;#39;&amp;#39;) %&amp;gt;% 
        str_replace_all(&amp;#39;\\[\\[&amp;#39;, &amp;#39;[&amp;#39;) %&amp;gt;%
        str_replace_all(&amp;#39;\\]\\]&amp;#39;, &amp;#39;]&amp;#39;)
## This is that pesky regex. If the movie is return on the run that ask for movies rated 0 to 9 on profanity, then we know it is 0-9 as the last digit...      
      if(j == 9){
        vector &amp;lt;- vector %&amp;gt;% 
          str_extract_all(&amp;#39;([^\\]]* \\[\\d{4}\\] \\[\\S+\\] - [0-9]{1,2}.[0-9]{1,2}.[0-9])&amp;#39;)

      ##...otherwise, we know the last number is a 10!
      } else {
        vector &amp;lt;- vector %&amp;gt;% 
          str_extract_all(&amp;#39;([^\\]]* \\[\\d{4}\\] \\[\\S+\\] - [0-9]{1,2}.[0-9]{1,2}.10)&amp;#39;)
      }
      movies_vector &amp;lt;- c(movies_vector,vector[[1]])
      
      position &amp;lt;- position + 34
      if(any(duplicated(movies_vector)) == TRUE) break
    }

## Also, I couldn&amp;#39;t figure out a good way to stop the loop. If you keep running it after the last movie, you&amp;#39;ll return to the start and begin again. So, as dumb as it is, I simply looked up the last movie in the list and wrote it down. Given that Zootopia starts with a Z and followed by an O, it is unlikely there will be many more movies after this. I&amp;#39;ll take my chances.
    
    if(any(grepl(&amp;#39;Zootopia&amp;#39;,movies_vector)) == TRUE) break
  }
  
 
  ## Some stringr FTW
  movies_df &amp;lt;- 
    as_data_frame(movies_vector) %&amp;gt;% 
    separate(value, sep = &amp;#39;\\[&amp;#39;, remove = TRUE, into = c(&amp;#39;name&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;rating&amp;#39;)) %&amp;gt;% 
    separate(rating, sep = &amp;#39;\\] - &amp;#39;, remove = TRUE, into = c(&amp;#39;mpaa&amp;#39;, &amp;#39;rating&amp;#39;)) %&amp;gt;% 
    separate(rating, sep = &amp;#39;\\.&amp;#39;, remove = TRUE, into = c(&amp;#39;sex&amp;#39;, &amp;#39;violence&amp;#39;, &amp;#39;profanity&amp;#39;))
 
  ## Some dplyr FTW
  movies_df &amp;lt;- movies_df %&amp;gt;% 
    mutate(
      year = str_replace(movies_df$year, &amp;#39;\\]&amp;#39;, &amp;#39;&amp;#39;),
      sex = as.numeric(sex),
      violence = as.numeric(violence),
      profanity = as.numeric(profanity),
      mpaa = factor(mpaa, levels = c(&amp;#39;G&amp;#39;, &amp;#39;PG&amp;#39;, &amp;#39;PG-13&amp;#39;, &amp;#39;R&amp;#39;, &amp;#39;NC-17&amp;#39;, &amp;#39;NR&amp;#39;))
    ) 
  
  ##Save output - I didn&amp;#39;t want to ping their website every time. 
  write_csv(movies_df, paste(path,&amp;#39;movies_extract.csv&amp;#39;, sep = &amp;quot;/&amp;quot;))
  remove(i,j,k, movies_vector, position, url, vector, import)
}

caption &amp;lt;- &amp;#39;\ntaraskaduk.com\nbased on data from kids-in-mind.com&amp;#39;



# Transform ---------------------------------------------------------------

movies_df &amp;lt;- movies_df %&amp;gt;% 
  filter(mpaa != &amp;#39;NR&amp;#39;) %&amp;gt;% ## Not Rated movies are NOT RATED
  filter(name != &amp;#39;Mozart\&amp;#39;s Sister&amp;#39;) %&amp;gt;%  ## This one is the one I caught that is wrong - it is actually NR and not G.
  mutate(avg = (sex + profanity + violence)/3)

## Not enough NC-17 movies - let&amp;#39;s match them up with R
movies_df$mpaa &amp;lt;- movies_df$mpaa %&amp;gt;% recode(R = &amp;#39;R &amp;amp; NC-17&amp;#39;, `NC-17` = &amp;#39;R &amp;amp; NC-17&amp;#39;) 

movies_df$mpaa &amp;lt;- factor(movies_df$mpaa, levels = c(&amp;#39;G&amp;#39;, &amp;#39;PG&amp;#39;, &amp;#39;PG-13&amp;#39;, &amp;#39;R &amp;amp; NC-17&amp;#39;))

movies_gather &amp;lt;- movies_df %&amp;gt;% 
  gather(key = metric, value = score, c(sex, violence, profanity, avg))


# Graphs -----------------------------------------------------------------

theme &amp;lt;- theme(
  legend.position=&amp;quot;none&amp;quot;,
  axis.ticks.y=element_blank(),
  panel.grid.major.y = element_line(colour=&amp;quot;#e0e0e0&amp;quot;,size=40),
  panel.grid.major.x =element_line(colour=&amp;quot;#F0F0F0&amp;quot;,size=.75),
  panel.grid.minor =element_blank(),
  panel.background=element_rect(fill=&amp;quot;#F0F0F0&amp;quot;),
  plot.background=element_rect(fill=&amp;quot;#F0F0F0&amp;quot;),
  plot.title=element_text(face=&amp;quot;bold&amp;quot;, colour=&amp;quot;#3C3C3C&amp;quot;,size=16),
  plot.subtitle=element_text(colour=&amp;quot;#3C3C3C&amp;quot;,size=12),
  plot.caption = element_text(colour=&amp;quot;#3C3C3C&amp;quot;,size=10),  
  axis.text.x=element_text(size=11,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;),
  axis.text.y=element_text(size=11,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;),
  axis.title.y=element_text(size=11,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;,vjust=1.5),
  axis.title.x=element_text(size=11,colour=&amp;quot;#535353&amp;quot;,face=&amp;quot;bold&amp;quot;,vjust=-0.5),
  plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), &amp;quot;cm&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;on-average-higher-mpaa-rating-follows-higher-levels-of-inappropriate-content-but&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;On average, higher MPAA rating follows higher levels of inappropriate content, but…&lt;/h3&gt;
&lt;p&gt;The first result seemed fairly obvious: higher (stricter) MPAA ratings have a higher rate of violence, sex and profanity. &lt;strong&gt;On average&lt;/strong&gt;. However, the amount of overlap is astonishing. Basically, any category is entirely consumed by its two neighboring categories.&lt;/p&gt;
&lt;p&gt;What’s more, you can always find a movie in a “lower” category that is more inappropriate than some other movie in a “higher” category: Jimmy Neutron VS Little Rascals, the 5th Harry Potter VS Life is Beautiful, Year One VS The King’s Speech etc.&lt;/p&gt;
&lt;p&gt;You can see this from the figure below. You may also notice that there movies scoring 2.5 points on average that are in every MPAA category. We’ll come back to this later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels_all &amp;lt;- movies_df %&amp;gt;% 
  filter(name == &amp;quot;Jimmy Neutron: Boy Genius&amp;quot; | 
           name == &amp;#39;Adventures of Elmo In Grouchland, The&amp;#39; |
           name == &amp;quot;Harry Potter and the Half-Blood Prince&amp;quot; |
           name == &amp;#39;Year One&amp;#39; |
           name == &amp;#39;Halloween&amp;#39; |
           name == &amp;#39;Little Rascals, The&amp;#39; |
           name ==  &amp;#39;Life is Beautiful&amp;#39; |
           name == &amp;#39;King\&amp;#39;s Speech, The&amp;#39;
  )

ggplot(data = movies_df, aes(x=avg, y = mpaa, col = mpaa)) +
  geom_jitter(alpha = 0.2) +
  scale_colour_brewer(palette = &amp;quot;Spectral&amp;quot;, direction = -1) +
  geom_point(data = labels_all, size = 3) +
  geom_label_repel(
    data = labels_all,
    aes(label = name),
    size = 3,
    nudge_y = 0.1) +
  ylab(&amp;#39;MPAA Rating&amp;#39;) + xlab(&amp;#39;Average (violence &amp;amp; gore, sex, profanity)&amp;#39;) +
  labs(
    title = &amp;#39;MPAA rating isn\&amp;#39;t everything&amp;#39;,
    subtitle = &amp;#39;Visualizing the amount of overlap between categories&amp;#39;,
    caption = caption) +
  theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/mpaa-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mpaa-is-most-forgiving-on-violence&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MPAA is most forgiving on violence&lt;/h3&gt;
&lt;p&gt;Well, no kidding! This was hardly a surprise. As a foreigner, I am constantly amused by how much violence is considered appropriate, contrasted with, for example, how little nudity is acceptable. &lt;em&gt;Guts and blood? Body parts?&lt;/em&gt; &lt;strong&gt;Sure, bring it on!&lt;/strong&gt; &lt;em&gt;Naked breasts?&lt;/em&gt; &lt;strong&gt;How dare you!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So, next time you rent a G rated movie and think it is clean - think again. It’s probably just as violent as that other PG movie you wanted. Both G and PG movies center around 3 points on violence anyway, with max points being 5 for G and 6 for PG. Just go with PG then, eh?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels_g_pg &amp;lt;- movies_df %&amp;gt;% 
  filter(name == &amp;quot;Babe: Pig in the City&amp;quot; | 
           name == &amp;quot;Beauty and the Beast&amp;quot; &amp;amp; mpaa == &amp;#39;G&amp;#39; |
           name == &amp;#39;Zeus and Roxanne&amp;#39; |
           name == &amp;#39;Sleepless in Seattle&amp;#39;)

ggplot(data = movies_df %&amp;gt;% filter(mpaa %in% c(&amp;#39;PG&amp;#39;, &amp;#39;G&amp;#39;)), aes(x=as.factor(violence), y = mpaa, col = as.factor(violence))) +
  geom_jitter(alpha = 0.5, size =3) +
  scale_colour_brewer(palette = &amp;quot;Spectral&amp;quot;, direction = -1) +
  geom_point(data = labels_g_pg, size = 5) +
  geom_label_repel(
    data = labels_g_pg,
    aes(label = name),
    size = 4,
    col = &amp;#39;grey51&amp;#39;,
    nudge_x = 1,
    nudge_y = 0) +
  theme +
  theme(axis.text.y = element_text(size = rel(1.2)),
        panel.grid.major.y = element_line(colour=&amp;quot;#e0e0e0&amp;quot;,size=70)) +
  ylab(&amp;#39;MPAA Rating&amp;#39;) + xlab(&amp;#39;Violence and Gore&amp;#39;) +
  labs(
    title = &amp;#39;That G movie you felt safe about&amp;#39;,
    subtitle = &amp;#39;is probably just as violent as the PG one you rejected&amp;#39;,
    caption = caption)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/violence-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-the-is-up-with-profanity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What the **** is up with profanity?&lt;/h3&gt;
&lt;p&gt;Now, this is a zero tolerance zone in the movie world. Not sex and nudity, as I assumed. Profanity. Unlike other categories, where scores flow gradually from category to category, profanity has some clear trends:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All G movies are bundled up in a narrow 0-2 points corridor&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Most PG-13 movies are between 4 and 5 points on profanity&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;R and NC-17 movies reside between 5 and 10 points&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I bet if I was trying to predict an MPAA rating based on these criteria, profanity would be the strongest predictor (not a concern of this post, but maybe later)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels_profanity &amp;lt;- movies_df %&amp;gt;% 
  filter(name %in% c(&amp;#39;Aladdin&amp;#39;,
                &amp;#39;Beethoven&amp;#39;,
                &amp;#39;Life is Beautiful&amp;#39;,
                &amp;#39;Psycho&amp;#39;,
                &amp;#39;Cars&amp;#39;,
                &amp;#39;Apollo 13&amp;#39;,
                &amp;#39;Nutty Professor, The&amp;#39;,
                &amp;#39;Straight Outta Compton&amp;#39;)
         )

ggplot(data = movies_df, aes(x=as.factor(profanity), y = mpaa, col = mpaa)) +
  geom_jitter(alpha = 0.2) +
  scale_colour_brewer(palette = &amp;quot;Spectral&amp;quot;, direction = -1) +
  geom_point(data = labels_profanity, size = 3) +
  geom_label_repel(
    data = labels_profanity,
    aes(label = name),
    size = 3,
    nudge_y = 0.1) +
  ylab(&amp;#39;MPAA Rating&amp;#39;) + xlab(&amp;#39;Profanity&amp;#39;) +
  labs(
    title = &amp;#39;Profanity rating patterns across MPAA categories&amp;#39;,
    caption = caption) +
  theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/profanity-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at R &amp;amp; NC-17 section, it is tempting to dive in a bit more. Let’s go!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels_r &amp;lt;- movies_gather %&amp;gt;% 
  filter(metric != &amp;#39;avg&amp;#39;) %&amp;gt;% 
  filter(metric == &amp;#39;violence&amp;#39; &amp;amp; 
           name %in% c(
             &amp;#39;Sex and the City&amp;#39;, 
             &amp;#39;Basic Instinct&amp;#39;, 
             &amp;#39;Saw&amp;#39;, 
             &amp;#39;Nightmare on Elm Street, A&amp;#39;) | 
           metric == &amp;#39;sex&amp;#39; &amp;amp; 
           name %in% c(
             &amp;#39;Reservoir Dogs&amp;#39;, 
             &amp;#39;Love Actually&amp;#39;, 
             &amp;#39;Basic Instinct&amp;#39;, 
             &amp;#39;American Pie&amp;#39;) |
           metric == &amp;#39;profanity&amp;#39; &amp;amp; 
           name %in% c(
             &amp;#39;Psycho&amp;#39;,  
             &amp;#39;Office Space&amp;#39;,
             &amp;#39;Pulp Fiction&amp;#39;,
             &amp;#39;Old School&amp;#39;,
             &amp;#39;Anchorman&amp;#39;,
             &amp;#39;Amelie&amp;#39;
           )       
  )


ggplot(
    data = movies_gather %&amp;gt;% 
        filter(
            mpaa == &amp;#39;R &amp;amp; NC-17&amp;#39; &amp;amp; 
            metric != &amp;#39;avg&amp;#39;), 
    aes(x=as.factor(score), y = metric, col = mpaa)) +
    geom_jitter(alpha = 0.3, size = 3) +
    scale_colour_manual(values = c(&amp;#39;#d7191c&amp;#39;)) +
    geom_point(data = labels_r, size = 4,  col = &amp;#39;black&amp;#39;) +
    geom_label_repel(
        data = labels_r,
        aes(label = name),
        size = 3,
        col = &amp;#39;black&amp;#39;) +
    guides(fill = FALSE) +
    xlab(&amp;#39;&amp;#39;) + ylab(&amp;#39;&amp;#39;) +
  theme +
  labs(
    title = &amp;#39;R &amp;amp; NC-17 Movies aren\&amp;#39;t always violent or vulgar... \nbut they sure are profane&amp;#39;,
    subtitle = &amp;#39;Most R and NC-17 movies are 5 or more on a profanity scale&amp;#39;,
    caption = caption)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/profanity2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Indeed, movies in R &amp;amp; NC-17 categories are widely distributed across violence and sex, but snuggle tightly in the upper section of profanity. Why is that? Looking at the data, we can tell that often profanity accompanies other “R” worthy content. However, it is not always the case, and correlation is relatively weak. &lt;a id=&#34;quote&#34;&gt;Good Will Hunting is neither violent nor sexually explicit, but it is profane AF, and, sure enough, is R rated for - wait for it - &lt;em&gt;“strong language, including some sex-related dialogue”.&lt;/em&gt; &lt;/a&gt; It could be just me (after all, I am a foreigner, and English words don’t carry the same connotation for me), but I think it is mighty unfair to Good Will Hunting to be rated R, especially knowing that Scary Movie, parts 3 through 5, are rated PG-13.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(
    data = movies_gather %&amp;gt;% 
        filter(metric != &amp;#39;avg&amp;#39;), 
    aes(x=as.factor(score), y = metric, col = mpaa)) +
    geom_jitter(alpha = 0.2, size = 2) +
    scale_colour_brewer(palette = &amp;quot;Spectral&amp;quot;, direction = -1) +
    geom_point(data = movies_gather %&amp;gt;% 
        filter(name == &amp;#39;Good Will Hunting&amp;#39; &amp;amp; metric != &amp;#39;avg&amp;#39;), 
        size = 7,  
        col = &amp;#39;grey20&amp;#39;,
        alpha = 0.9) +
     geom_point(data = movies_gather %&amp;gt;% 
        filter(name == &amp;#39;Good Will Hunting&amp;#39; &amp;amp; metric != &amp;#39;avg&amp;#39;), 
        size = 2,  
        col = &amp;#39;grey10&amp;#39;) +
    guides(fill = FALSE) +
    xlab(&amp;#39;&amp;#39;) + ylab(&amp;#39;&amp;#39;) +
  theme +
  labs(
    title = &amp;#39;Good Will Hunting, rated R&amp;#39;,
    subtitle = &amp;#39;For strong language, including some sex-related dialogue&amp;#39;,
    caption = caption)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mpaa/mpaa_files/figure-html/good_will_hunting-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;So, what have we learned?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is probably OK to use MPAA ratings as a guide&lt;/li&gt;
&lt;li&gt;If you’re optimizing for lack of violence, G and PG movies aren’t that much different, therefore don’t worry much.&lt;/li&gt;
&lt;li&gt;R rating doesn’t mean the movie is violent or has a lot of sexual content. But it definitely means there is some profanity in it!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Caveats&lt;/h1&gt;
&lt;p&gt;It is important to remember that any rating will be arbitrary a priori. We aren’t working with exact count of swear words, time duration of violent scenes, or percentage of naked body revealed. And &lt;a href=&#34;http://kids-in-mind.com&#34;&gt;kids-in-mind.com&lt;/a&gt; rating isn’t perfect either. For example, the website rates Pulp Fiction at 10 on a “sex &amp;amp; nudity” scale, while there is hardly any sexual content in the movie.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Outro&lt;/h1&gt;
&lt;p&gt;This post was nothing but an exercise in &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;blogdown&lt;/code&gt;, &lt;code&gt;ggplot&lt;/code&gt; and &lt;code&gt;rvest&lt;/code&gt; packages, along with honing my data storytelling and writing skills though. I am sure if I torture this dataset a bit more, it may confess to many other things. So, I will wrap up for now, but plan to return to this topic and this dataset in the future. Let me know if there are any specific questions that come to your mind for this dataset!&lt;/p&gt;
&lt;div id=&#34;r-script&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R script&lt;/h3&gt;
&lt;p&gt;This site runs on &lt;del&gt;dunkin&lt;/del&gt; &lt;code&gt;blogdown&lt;/code&gt;, and therefore all the source files are available on GitHub. Now, I must be honest: I am new to Github, to Hugo, and to the web in general. I have very little idea about what I’m doing. But I guess here is the link? &lt;a href=&#34;https://github.com/taraskaduk/taraskaduk/tree/master/content/post/movies&#34; class=&#34;uri&#34;&gt;https://github.com/taraskaduk/taraskaduk/tree/master/content/post/movies&lt;/a&gt; (unless I change something or screw it up big time, in which case throw rocks at me in the comment section)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Migrate your enterprise reporting into Power BI: process canvas, pitfalls &amp; best practices</title>
      <link>/talk/sql-sat-649/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>/talk/sql-sat-649/</guid>
      <description>&lt;p&gt;In this session, we will discuss how to efficiently migrate your retrospective analytics from where it is now to Power BI, and do it in a way that saves you time in the future.&lt;/p&gt;

&lt;p&gt;Some of the topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How to create a solution that is simple, automated, standardized and scalable&lt;/li&gt;
&lt;li&gt;How to avoid common pitfalls and &amp;ldquo;death by a thousand reports&amp;rdquo;&lt;/li&gt;
&lt;li&gt;How to most efficiently distribute the reports afterwards&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>/project/deep-learning/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/deep-learning/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/project/example-external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/example-external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Person Re-Identification System For Mobile Devices</title>
      <link>/publication/person-re-identification/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/publication/person-re-identification/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mobile visual clothing search</title>
      <link>/publication/clothing-search/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>/publication/clothing-search/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
